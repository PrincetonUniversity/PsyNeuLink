{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 0.3 Perceptron and XOR\n",
    "\n",
    "![perceptron](perceptron.png)\n",
    "\n",
    "##  Introduction\n",
    "\n",
    "To first approximation, activity of a neuron in the brain is determined by the integration of excitatory and inhibitory impulses received by its dendrites and passed to the cell body. If excitatory signals outweigh inhibitory signals sufficiently to pass a threshold, the neuron will fire, sending out an action potential via its axon.  \n",
    "  \n",
    "Artificial neurons were conceived to behave in a similar manner to real ones. Early artificial neurons were dubbed perceptrons; they received multiple binary inputs, multiplied each one by an appropriate “weight”, added them together (this should sound familiar), and produced a single binary output that depends on whether the sum passes a certain threshold.  In short, they applied a step function to the transformed input.  This is referred to as an LBF (linear basis function) activation function.  \n",
    "  \n",
    "![LBF](lbf_perceptron.jpeg)\n",
    "  \n",
    "If we define the threshold $ \\equiv-b $, then we can rewrite our conditional output  \n",
    "  \n",
    "$$\n",
    "\\begin{equation}\n",
    " output=\n",
    "\\begin{cases}\n",
    " 0 & if \\: w \\cdot x + b \\leq 0\n",
    "\\\\\n",
    "1  & if \\: w \\cdot x + b >  0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "  \n",
    "The term $b$ is called a bias, and perceptrons are a special kind of linear classifier called a binary classifier. It is known as a binary classifier because it is assigning one of two labels (the binary output) to the inputs it receives, according to their dot product with the weights (a linear operation). The line defined by $ w \\cdot x+b=0 $ is the line that separates the classes from each other. Our “red square, blue circle” class example from earlier in the lab is an example of binarily classifiable data.  \n",
    "  \n",
    "![multilayer ffn](multilayer_ffn.jpeg)\n",
    "  \n",
    "In a multilayer feedforward network, artificial neurons are clustered into layers, where the output of one layer forms the input of the next. The first layer is known as the input layer, the last is the output layer, and the operational layers, where the dot products are computed, are called “hidden layers”.\n",
    "\n",
    "Binary classification is simple but powerful. We will show this by using them to construct a set of logic gates."
   ],
   "id": "ea1be4183f9d8c51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Installation and Setup**",
   "id": "20034bcc17269812"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "%pip install psyneulink\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import psyneulink as pnl"
   ],
   "id": "569b0ad66e915ffc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## AND Gate\n",
    "\n",
    "An AND gate is a logic gate that answers the question: \n",
    "\n",
    "“Are A and B both simultaneously true?”. \n",
    "\n",
    "In this context, we represent \"True\" with `1` and \"False\" with `0`.\n",
    "\n",
    "\n",
    "First, let's define a *training set* that includes all possible input combinations of the boolean values True (`1`) and False (`0`) and their corresponding *labels*. Remember, AND(X, Y) is true if and only if both X and Y are true.\n",
    "\n",
    "***Note:*** In a machine learning context, we often refer to a *training set* as a collection of input-output pairs used to teach a model. A *label* as the correct output for a given input."
   ],
   "id": "a1166245100c1c59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create the training set\n",
    "training_set = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "training_set = training_set.T\n",
    "n_combos = np.shape(training_set)[1]\n",
    "ub = np.max(abs(training_set))\n",
    "lb = - ub\n",
    "\n",
    "# Create the labels\n",
    "and_labels = np.array([0, 0, 0, 1])\n",
    "and_labels.shape = (1, n_combos)\n",
    "\n",
    "# \n",
    "# Reshape labeled_set to match a 2D grid\n",
    "labeled_set = np.zeros((2, 2))  # 2x2 grid for (A, B) inputs\n",
    "for i in range(n_combos):\n",
    "    A, B = training_set[:, i]  # Extract each (A, B) pair\n",
    "    labeled_set[B, A] = and_labels[0, i]  # Assign label at corresponding (B, A) position\n",
    "\n",
    "# Plot the labeled set\n",
    "plt.figure()\n",
    "plt.title('Boolean Matrix')\n",
    "plt.xticks(np.arange(0, n_combos))\n",
    "plt.yticks(np.arange(0, 2))\n",
    "plt.imshow(labeled_set, cmap='RdBu_r', vmin=lb, vmax=ub)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "id": "5739e8148c0e008e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Using Weights in a Perceptron\n",
    "\n",
    "Given an input vector X (e.g., the values of A and B in an AND gate), the perceptron computes a weighted sum of these inputs using a weight matrix W and then applies a bias term to adjust the threshold for activation:\n",
    "\n",
    "$$\n",
    "Z = Wx + b\n",
    "$$\n",
    "\n",
    "\n",
    "Z is the weighted sum before applying an activation function.\n",
    "This weighted sum determines the perceptron’s output. If Z is greater than a certain threshold (often 0), the perceptron outputs 1 (True); otherwise, it outputs 0 (False).\n",
    "\n",
    "Here, we implement a random weight matrix with a fixed bias (`b`=-1):\n"
   ],
   "id": "563fe1f657083806"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(16)\n",
    "\n",
    "# Remember, the shape of the matrix is determined by its input shape (2 in this case), and its output shape (1 in this case)\n",
    "w = np.random.rand(1, 2)\n",
    "\n",
    "# Set the bias\n",
    "b = -1\n",
    "\n",
    "# define a function that calculates the output of the perceptron\n",
    "def predict(x):\n",
    "    # calculate the weighted sum\n",
    "    z = w @ x + b\n",
    "    # check weather z is above the threshold\n",
    "    return int(z[0] > 0)\n",
    "\n",
    "# Let's print the outputs of this random perceptron on the training set:\n",
    "predicted_set = np.zeros((2, 2))  # 2x2 grid for (A, B) inputs\n",
    "for i in range(n_combos):\n",
    "    A, B = training_set[:, i]  # Extract each (A, B) pair\n",
    "    predicted_set[B, A] = predict(training_set[:, i])\n",
    "      # Assign label at corresponding (B, A) position\n",
    "\n",
    "\n",
    "# Plot the predictions\n",
    "plt.figure()\n",
    "plt.title('Boolean Matrix')\n",
    "plt.xticks(np.arange(0, n_combos))\n",
    "plt.yticks(np.arange(0, 2))\n",
    "plt.imshow(predicted_set, cmap='RdBu_r', vmin=lb, vmax=ub)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "id": "7332e1a9d5769df0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Exercise 1{exercise}\n",
    "\n",
    "The above implementation does not give the correct answer (it is not a and gate). Can you think about an algorithm that automatically \"finds\" the right weights?"
   ],
   "id": "891be3c668d2c8c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Hint{hint}\n",
    "\n",
    "You can be inspired by the [bogo sort algorithm](https://www.google.com/search?sca_esv=053903a76464d342&rlz=1C5GCCM_en&sxsrf=AHTn8zq_1Y3AoSWLlwKQhmfqMA_a_arNDA:1738512836603&q=pogo+sort&udm=7&fbs=ABzOT_CWdhQLP1FcmU5B0fn3xuWp6IcynRBrzjy_vjxR0KoDMs-NPkvpfnC0-Nvsd7HFz32b6_vVeH3Eh5LTn6vYQxgOy79dkH6TeiZNM9aLPSHRcxYTXxi8SKQDYJDO9qd6-gcAKe7AlCX8DiGxWpVr1AWEK_st4_2sOqSvy2q_T0FuN3a9CsFYiJFOmc5QD4AFJJ2g-eL4OU0xkk0cfwD6gzBQ2Mgs0g&sa=X&ved=2ahUKEwiotsCAsaWLAxWcF2IAHdigLm0QtKgLegQIGRAB&biw=1728&bih=870&dpr=2#fpstate=ive&vld=cid:9ee528e0,vid:DaPJkYo2quc,st:0). It randomly picks permutations, and tests if they are sorted."
   ],
   "id": "58f840aac400968"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Solution{solution}\n",
    "\n",
    "A simple (brute force) solution is to randomly initialize the weights until we find fitting ones:\n",
    "\n",
    "```python\n",
    "predicted_set = np.zeros((2, 2))\n",
    "tries = 0\n",
    "\n",
    "\n",
    "def predict(x, w):\n",
    "    z = w @ x + b\n",
    "    return int(z[0] >= 0)\n",
    "\n",
    "\n",
    "while not np.allclose(predicted_set, labeled_set):  # as long as our predictions doesn't match the labels\n",
    "    tries += 1\n",
    "    # get new random weights\n",
    "    w = np.random.rand(1, 2)\n",
    "    b = -1\n",
    "\n",
    "    for i in range(n_combos):\n",
    "        A, B = training_set[:, i]\n",
    "        predicted_set[B, A] = predict(training_set[:, i], w)\n",
    "        \n",
    "\n",
    "print(f\"Found fitting weights after {tries} tries:\")\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Boolean Matrix')\n",
    "plt.xticks(np.arange(0, n_combos))\n",
    "plt.yticks(np.arange(0, 2))\n",
    "plt.imshow(predicted_set, cmap='RdBu_r', vmin=lb, vmax=ub)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "```"
   ],
   "id": "1a1d04f21aaf33e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Learning the Weights\n",
    "\n",
    "For a simple gate like this, we could choose our weights and bias manually to correctly classify each example. However, we would rather the perceptron learn the weights itself. We can train it using supervised learning by showing it examples of the data with the appropriate labels (a truth table). If the weights were initialized randomly, these will generate a random answer at first, so the perceptron must change its weights when its output does not match the labels we gave it. \n",
    "  \n",
    "For perceptrons, we use error-based learning, where the weights are adjusted in the correct direction, based on the size and direction of the error. This is given by the Perceptron Learning Rule, and is written mathematically as:  \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "w_{ij} \\rightarrow w_{ij}+ \\eta (desired \\ label - predicted \\ label) in_{i}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where $w_{ij}$ is the weight connecting the $i-th$ input to the $j-th$ output, $in_{i}$ is the $i{th}$ input, and $ \\eta $ is the learning rate. The learning rate is a positive parameter that determines the size of the weight update."
   ],
   "id": "206ec195780d2073"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initializing the weights with random values.\n",
    "w = np.random.rand(1, 2) * .1\n",
    "\n",
    "# Set the bias and learning rate\n",
    "b = -1\n",
    "eta = np.array([.3, .3])\n",
    "\n",
    "# Variables to store the accuracy and loop count\n",
    "accuracy = 0\n",
    "loop_count = 0\n",
    "acc_vec = []\n",
    "\n",
    "while accuracy < 100:\n",
    "    summed_accuracy = 0\n",
    "    # Calculate the predicted value for each input and learn the weights \n",
    "    for i in range(np.shape(training_set)[1]):\n",
    "        # Set the predicted label to 0 if the predicted value is less than or equal to 0 \n",
    "        # and 1 otherwise\n",
    "        if w @ training_set[:, i] + b <= 0:\n",
    "            predicted_label = 0\n",
    "        else:\n",
    "            predicted_label = 1\n",
    "        # Check if the predicted label is correct\n",
    "        if predicted_label == and_labels[:, i]:\n",
    "            # Summing the accuracy\n",
    "            summed_accuracy = summed_accuracy + 100\n",
    "        else:\n",
    "            diff = (and_labels[:, i] - predicted_label)\n",
    "            # Update the weights if the predicted label is incorrect\n",
    "            w = w + eta * diff * training_set[:, i].T\n",
    "    accuracy = int(summed_accuracy / n_combos)\n",
    "    acc_vec = np.append(acc_vec, accuracy)\n",
    "    loop_count = loop_count + 1\n",
    "lc = loop_count\n",
    "\n",
    "# Plot the accuracy over time\n",
    "plt.figure()\n",
    "plt.plot(acc_vec)\n",
    "plt.title('Accuracy V. Loop Number')\n",
    "plt.xlabel('Loop Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(np.arange(0, int(lc)))\n",
    "plt.yticks([0, 25, 50, 75, 100])\n",
    "plt.show()"
   ],
   "id": "9664167d251ecd98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Exercise 1{exercise}\n",
    "\n",
    "Implement the OR gate using the perceptron learning rule. The OR gate is a logic gate that answers the question, “is A or B true?”. Remember, OR(X, Y) is true if either X or Y (or both) are true."
   ],
   "id": "3a0015e06d0ecb7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Implement your code here",
   "id": "aa692ba329eecd66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Solution 1{solution}\n",
    "\n",
    "```python\n",
    "# We can use the same input_set but use different labels\n",
    "or_labels = np.array([0, 1, 1, 1])\n",
    "or_labels.shape = (1, n_combos)\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initializing the weights with random values.\n",
    "w = np.random.rand(1, 2) * .1\n",
    "\n",
    "\n",
    "# Set the bia and learning rate\n",
    "b = -1\n",
    "eta = np.array([.3, .3])\n",
    "\n",
    "# Variables to store the accuracy and loop count\n",
    "accuracy = 0\n",
    "loop_count = 0\n",
    "acc_vec = []\n",
    "\n",
    "while accuracy < 100:\n",
    "    summed_accuracy = 0\n",
    "    # Calculate the predicted value for each input and learn the weights \n",
    "    for i in range(np.shape(input_set)[1]):\n",
    "        # Set the predicted label to 0 if the predicted value is less than or equal to 0 \n",
    "        # and 1 otherwise\n",
    "        if w @ input_set[:, i] + b <= 0:\n",
    "            predicted_label = 0\n",
    "        else:\n",
    "            predicted_label = 1\n",
    "        # Check if the predicted label is correct\n",
    "        if predicted_label == or_labels[:, i]:\n",
    "            # Summing the accuracy\n",
    "            summed_accuracy = summed_accuracy + 100\n",
    "        else:\n",
    "            diff = (or_labels[:, i] - predicted_label)\n",
    "            # Update the weights if the predicted label is incorrect\n",
    "            w = w + eta * diff * input_set[:, i].T\n",
    "    accuracy = int(summed_accuracy / n_combos)\n",
    "    acc_vec = np.append(acc_vec, accuracy)\n",
    "    loop_count = loop_count + 1\n",
    "lc = loop_count\n",
    "\n",
    "# Plot the accuracy over time\n",
    "plt.figure()\n",
    "plt.plot(acc_vec)\n",
    "plt.title('Accuracy V. Loop Number')\n",
    "plt.xlabel('Loop Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(np.arange(0, int(lc)))\n",
    "plt.yticks([0, 25, 50, 75, 100])\n",
    "plt.show()\n",
    "```"
   ],
   "id": "261a846b45ce1ba8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This method works well for a single layer of perceptrons. Now, consider multiple layers of perceptrons, each operating linearly, but producing a step function output. When they produce an incorrect result, how do we know which layer is responsible for the mistake and what step size would be useful to correct it?\n",
    "  \n",
    "Because of their binary output, linearly updating weights of perceptrons in a multilayer network produces unpredictable and problematic results. A natural solution is to replace the step function with a function that more gradually interpolates between 0 and 1. A good choice here is the [logistic function](https://princetonuniversity.github.io/NEU-PSY-502/content/Primers/notebooks/2%20Logistic%20Function.ipynb), which we have encountered in the previous section. It is possible to generalize the Perceptron Learning Rule to this case using calculus; the details are presented in the Appendix at the end of this notebook. We suggest that you skim the Appendix now, and then read it more carefully after you have finished this lab. Once you understand this derivation, you will be in good shape for when we discuss the Backpropagation algorithm in the next lab."
   ],
   "id": "27b1b2fe4c3083f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  Two Layer Network in PsyNeuLink\n",
    "\n",
    "It is reasonably straightforward to create a two layer logistic network in numpy. However, doing so in PNL is almost trivial. Here, we implement both the AND, and the OR gate in psyneulink. In addition, we also implement the XOR gate. Remember, XOR(X, Y) is true if X or Y is true, but not both. \n",
    "\n",
    "***Note:***, a XOR gate is not linearly separable, so a single layer perceptron cannot learn it. However, a two-layer network can learn it."
   ],
   "id": "46df9c823a9cb589"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trials = 10_000\n",
    "# To give the network extra flexibility, we include an extra dimension in the input whose value is always equal to 1\n",
    "# this effectively allows the network to learn a bias term\n",
    "\n",
    "input_set_pnl = [[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]]\n",
    "input_length = np.shape(input_set_pnl)[1]\n",
    "n_combos_pnl = np.shape(input_set_pnl)[0]\n",
    "\n",
    "# Note, PNL is capable of training biases, but to keep this tutorial simple using the method we will be incorporating for the\n",
    "# following exercises, the biases implemented by PNL would not be trained.\n",
    "# To resolve that issue, we implement the bias as an extra input, and train it as we would any other weight.\n",
    "\n",
    "and_labels_pnl = [[0], [0], [0], [1]]\n",
    "or_labels_pnl = [[0], [1], [1], [1]]\n",
    "xor_labels_pnl = [[0], [1], [1], [0]]\n",
    "\n",
    "# Specify which label set you would like to use.\n",
    "labels_pnl = and_labels_pnl  # Change this to OR_labels_pnl or XOR_labels_pnl to see the network learn the OR or XOR gate\n",
    "\n",
    "# Creating a 2 layer net in PNL:\n",
    "# First, we create the input layer. This layer is simply a Transfer Mechanism that brings the examples into the network\n",
    "# We do not have to specify a function (it defaults to linear, slope = 1, intercept = 0), \n",
    "# but we do need to specify the size, which will be the size of our input array.\n",
    "in_layer = pnl.TransferMechanism(input_shapes=input_length, function=pnl.Linear)\n",
    "\n",
    "# Next, we specify the output layer. This is where we do our logistic transformation by applying the Logistic function.\n",
    "# The size we specify for this layer is the number of output. In this case, we only have one output.\n",
    "out_layer = pnl.TransferMechanism(input_shapes=1, function=pnl.Logistic)\n",
    "\n",
    "# Finally, we create a projection mapping from input to output. We will initialize with random weights.\n",
    "weights = pnl.MappingProjection(name='in_to_out',\n",
    "                                matrix=np.random.rand(input_length, 1),\n",
    "                                sender=in_layer,\n",
    "                                receiver=out_layer)\n",
    "\n",
    "# The, we will put them together into an Autodiff Composition. These compositions are a faster option\n",
    "# for backpropagation learning that integrate PNL and pytorch.\n",
    "logic_two_layer = pnl.AutodiffComposition(\n",
    "    learning_rate=10,\n",
    ")\n",
    "\n",
    "# We will now add our layers and projection map into our composition\n",
    "logic_two_layer.add_node(in_layer)\n",
    "logic_two_layer.add_node(out_layer)\n",
    "\n",
    "# Here, we set the log conditions for the output layer. This will allow us to see the output of the network as it trains.\n",
    "out_layer.log.set_log_conditions(pnl.VALUE)\n",
    "\n",
    "# We add the projection to the composition\n",
    "logic_two_layer.add_projection(sender=in_layer, projection=weights, receiver=out_layer)\n",
    "\n",
    "# To learn our desired gates, we train the autodiff composition by giving it an input dictionary and running it\n",
    "input_dict = {\"inputs\": {in_layer: input_set_pnl}, \"targets\": {out_layer: labels_pnl}, \"epochs\": 50}\n",
    "result = logic_two_layer.learn(inputs=input_dict)\n",
    "\n",
    "# Here, we acquire a log of the losses over time so we can see how our network learned\n",
    "#This portion acquires and plots the losses\n",
    "exec_id = logic_two_layer.default_execution_id\n",
    "losses = logic_two_layer.parameters.torch_losses.get(exec_id)\n",
    "\n",
    "print(\"The last loss was \", losses[-1])\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title('Losses of 2 layer net in PNL as a function of trials')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "#This portion acquires and plots the labels\n",
    "\n",
    "# The logged values of the output layer that were recorded during the training\n",
    "data = out_layer.log.nparray()[1][1][-1][1:]\n",
    "\n",
    "# Psyneulink stores the logged values as a list of length 200 (50 epochs x 4 inputs per epoch)\n",
    "# We will reshape this list into an array of shape 50 x 4, so that each column represents one of the training patterns\n",
    "# and each row represents one training epoch\n",
    "length = np.shape(data)[0]\n",
    "rat = int(length / n_combos_pnl)\n",
    "\n",
    "data = np.reshape(data, (rat, n_combos_pnl))\n",
    "data = np.matrix(data)\n",
    "labs_1 = np.array(data[:, 0])\n",
    "labs_2 = np.array(data[:, 1])\n",
    "labs_3 = np.array(data[:, 2])\n",
    "labs_4 = np.array(data[:, 3])\n",
    "\n",
    "plt.plot(labs_1, label='first pattern', color='red')\n",
    "plt.plot(labs_2, label='second pattern', color='blue')\n",
    "plt.plot(labs_3, label='third pattern', color='green')\n",
    "plt.plot(labs_4, label='fourth pattern', color='black')\n",
    "plt.legend()\n",
    "\n",
    "plt.title('labels 1 through 4 versus epoch number')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Label Value')\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "ab93fcbcb4850224",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Explore the effectiveness of this 2 layer network by running it at different learning rates, for different numbers of trials, and on different label sets. \n",
    "\n",
    "Which sets does it learn effectively? Which sets doesn't it?"
   ],
   "id": "eaf83f103a32c353"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 2{exercise}\n",
    "\n",
    "Try the running the two layer network on the XOR gate. You will observe that the network is not able to learn the XOR gate. Why do you think this is the case?\n",
    "\n",
    "Hint: Consider the following graph that shows the values of the XOR gate for different inputs:"
   ],
   "id": "4f567088a9e77b3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert to numpy arrays, as required for plotting functions\n",
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "# We will plot each point as red if the XOR relation is satisfied, and blue if not \n",
    "xor_colors = ['blue', 'red', 'red', 'blue']\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.scatter(x[:, 0], x[:, 1], [1, 1, 1, 1], c=xor_colors)\n",
    "plt.title('XOR projected into 3-d')\n",
    "plt.show()"
   ],
   "id": "e216c40c86fe8ab5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Solution 2{solution}\n",
    "\n",
    "The important difference between the XOR compared to AND, and OR is that XOR is not *linearly separable*, while the other two are. This means that in the graph above, that it is not possible to draw a straight line such that \n",
    "\n",
    "1.   Both red points are on the same side of the line\n",
    "2.   Both blue points are on the same side of the line\n",
    "3.   The red points are on the opposite side as the blue points. \n",
    "\n",
    "In 2 dimensions, this is clearer. Try running the following code to see the XOR gate in 2 dimensions (copy it into a code cell) and create a random line. You can convince yourself that no matter which line is chosen, at least one of the three conditions above will be violated.\n",
    "\n",
    "```python\n",
    "#the third dimension is equal to 1 for all four points, so it makes no difference to ignore it\n",
    "plt.scatter(x[:,0],x[:,1],c=xor_colors)\n",
    "\n",
    "#randomly generate and plot a line \n",
    "y_intercept=np.random.uniform(low=-2,high=2)\n",
    "x_intercept=np.random.uniform(low=-2,high=2)\n",
    "\n",
    "slope=-y_intercept/x_intercept\n",
    "plt.plot([0,1],[y_intercept,y_intercept+slope])\n",
    "```\n",
    "\n",
    "In contrast, for the AND gate, a line that separates the blue and red dots exists:\n",
    "\n",
    "```python\n",
    "and_colors=['blue','blue','blue','red']\n",
    "\n",
    "\n",
    "plt.scatter(x[:,0],x[:,1],c=and_colors)\n",
    "\n",
    "y_intercept=1.5\n",
    "x_intercept=1.5\n",
    "\n",
    "slope=-y_intercept/x_intercept\n",
    "plt.plot([0,1],[y_intercept,y_intercept+slope])\n",
    "```\n",
    "\n",
    "***Optional:*** Convince yourself that the OR gate is also linearly separable by plotting it in 2 dimensions.\n",
    "\n",
    "Now, why is linear separability important here? On the one hand, our network is defined so that when it is given an input $x$, it produces an output of $1$ if $w\\cdot x+b>0$ and an output of 0 otherwise. It will be able to learn effectively provided that there exist some $w$ and $b$ such that $w\\cdot x+b>0$ whenever $x$ is red and $<0$ whenever $x$ is blue (since we want the output to be positive whenever $x$ is red and negative when blue).\n",
    " \n",
    "  On the other hand, consider a line with a y-intercept of $y_0$ and a slope of $m$. Then a point $(a,b)$ lies above this line if $b>ma+y_0$ and lies below the line otherwise. We can rearrange this condition to read $(-m,1)\\cdot (a,b) -y_0>0$ which is the same as the first equation,  provided we take $w=(-m,1)$, $b=-y_0$, and $x=(a,b)$. This means that our original condition $w\\cdot x+b>0$ is equivalent to checking whether $x$ is above or below some line (with the parameters of the line being determined by $w$ and $b$, and vice versa). So in order for our network to learn effectively, it must be the case that all the red points are above some line, and all the blue points are below that same line, i.e. the points must be linearly separable. Since XOR does not satisfy this condition, our network cannot learn the labeling in this case. "
   ],
   "id": "fa7f27df8311a2a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Appendix: Derivation of Learning Rule for two-layer network with sigmoidal activation\n",
    "\n",
    "Sigmoid neurons take inputs and produce outputs similar to perceptrons, however, the inputs and outputs are not binary. Additionally, rather than applying a simple dot product to the inputs such that\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "z(w,x) = w \\cdot x + b\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "these new types of neurons apply the sigmoid function, of the form\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma(z(w,x))=\\frac{1}{1+e^{-z(w,x)}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This function is actually the same as a logistic function with a bias value of 0 and a gain value of 1. It has an upper bound at 1 and a lower bound at zero. This can be seen by examining the limits: $\\lim_{z(w,x)\\to\\infty} \\sigma(z(w,x))$ and $\\lim_{z(w,x)\\to -\\infty} \\sigma(z(w,x))$. Although its equation looks complex, a logistic function can quickly become intuitive when you try modifying the parameters, as shown in [logistic function](https://princetonuniversity.github.io/NEU-PSY-502/content/Primers/notebooks/2%20Logistic%20Function.ipynb).\n",
    "\n",
    "In a multilayer network, the output of multiple sigmoid functions in the first non-input layer forms the input to the next layer. However, because of its bounded structure, a sigmoid function only outputs 1 or 0 at the limits (when rounding error kicks in). Typically its output will be somewhere in between. Therefore, there will always be some error between the desired output of a logical function (1 or 0) and the output of a sigmoid. The error of a single output is given by  \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E=desired \\ output - \\sigma(z(w,x))\n",
    "\\end{equation}\n",
    "$$\n",
    "  \n",
    "A common measure of error is called the mean squared error, and is given by  \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E =\n",
    "\\frac{1}{n} \\ \\sum_{i=1}^{n} (desired \\ output_{i} - \\sigma(z(w,x))_{i})^2\n",
    "\\end{equation}\n",
    "$$ \n",
    "  \n",
    "Although we cannot drive this value to zero (when our desired output is binary), we can minimize it. Because it is a sum of squares, we can minimize it by minimizing each term. Because each term is a convex function of the weights, it is minimized when its derivative as a function of the weights is zero. We can train our neural net to do this using the delta rule, which is very similar to the perceptron learning rule. \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "w^{t+1} = \\underbrace{w^t}_\\text{current weight} + \\underbrace{\\Delta w^t}_\\text{weight change}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\Delta w^t = - \\alpha \\underbrace{\\frac{\\partial E^t}{\\partial w^t}}_\\text{change in error terms as a function of the weights}\n",
    "\\end{equation}\n",
    "$$\n",
    "  \n",
    "As you can see, when the derivative of the error is zero, the weights stop changing. The error is, at this point, minimized.  \n",
    "  \n",
    "In order to calculate the derviative of the error, we simply employ the chain rule. We will aslo be using a modified form of the error. Instead of scaling each term by $\\frac{1}{n}$, we will scale by $\\frac{1}{2}$ \n",
    "\n",
    "So, each error term is given by \n",
    " \n",
    "$$\n",
    "\\begin{equation}\n",
    "E_{i} =\n",
    "\\frac{1}{2} (desired \\ output_{i} - \\sigma(h(x))_{i})^2\n",
    "\\end{equation} \n",
    "$$\n",
    "  \n",
    "and its derivative is given by  \n",
    " \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E_{i}}{\\partial w} = \\underbrace{\\frac{\\partial E_{i}}{\\partial \\sigma(h(w,x))}}_\\text{derivative 1} \\quad\n",
    "\\underbrace{\\frac{\\partial \\sigma(h(w,x))}{\\partial h(w,x)}}_\\text{derivative 2} \\quad\n",
    "\\underbrace{\\frac{\\partial h(w,x)}{\\partial w}}_\\text{derivative 3}\n",
    "\\end{equation}\n",
    "$$\n",
    "  \n",
    "Now, because $h(w,x)$ is a vector equation, its derivative is also a vector. This produces our vector of weight changes, each term of which is determined by derivative 3  \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\quad\n",
    "\\underbrace{\\frac{\\partial h(w,x)}{\\partial w_{i}}}_\\text{derivative 3}\n",
    "\\end{equation}  \n",
    "$$\n",
    "\n",
    "This comes out surprisingly tidily:  \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "{\\frac{\\partial E^t}{\\partial w_{i}^t}}=(desired \\ output - \\sigma) (\\sigma (1 - \\sigma))x_{i}\n",
    "\\end{equation}\n",
    "$$\n",
    "  \n",
    "Now\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\Delta w^t_{i} = - \\alpha (desired \\ output - \\sigma) (\\sigma (1 - \\sigma))x_{i}\n",
    "\\end{equation}\n",
    "$$\n",
    "  \n",
    "Biases are updated in much the same fashion, using\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\Delta b^t_{i} = - \\alpha (desired \\ output - \\sigma) (\\sigma (1 - \\sigma))1\n",
    "\\end{equation}\n",
    "$$\n"
   ],
   "id": "b2f4b5fbe711b348"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "and_colors=['blue','blue','blue','red']\n",
    "\n",
    "\n",
    "plt.scatter(x[:,0],x[:,1],c=and_colors)\n",
    "\n",
    "y_intercept=1.5\n",
    "x_intercept=1.5\n",
    "\n",
    "slope=-y_intercept/x_intercept\n",
    "plt.plot([0,1],[y_intercept,y_intercept+slope])"
   ],
   "id": "ff3d5cfec1c792dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "67ccdd9eca719884",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
