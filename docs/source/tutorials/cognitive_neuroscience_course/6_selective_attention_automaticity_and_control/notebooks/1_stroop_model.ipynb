{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6.1 Stroop Model\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Pat your head and rub your belly at the same time.  Unless you have practiced this odd exercise you will get conflict between the motor commands and quickly end up rubbing your head while also rubbing your belly, OR patting your belly while also patting your head. As a more difficult exercise, while sitting, lift your right foot and repeatedly rotate it clockwise while simultaneously tracing a counterclockwise circle in the air with your right hand. Contrast the difficulty of this task with a contralateral attempt, using your left foot and right hand (or right hand and left foot). Action is an obvious bottleneck -- if you see a threat and try to both \"flight\" and \"fight\" at the very same time, the results will look funny and be ineffective. Some of our cognitive capacity limitations could be byproducts of the need to select singular coherent plans of action. In this lab we will begin thinking about how multiple psychological processes combine and interact, and how to model what happens when signals and processing conflict. A classic example is the Stroop task.\n"
   ],
   "id": "c70e1b82145057df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attention, Automaticity, & Control\n",
    "\n",
    "Each of your eyes has about 120 million light-sensitive receptors on the retina, receiving input around 10 times per second.  Between your two eyes and all your other sensory receptors (smell, taste, touch, sound), you are receiving many billions of units of stimulation every second.  Not all of this information makes it into your brain -- for example, before information exits your eye it has already been processed and compressed down to the firing output of 10 million retinal ganglion cells.  Still this is a torrent of information, and you can only be consciously aware and act upon a tiny fraction of all the incoming signals.  Attention is a collection of processing mechanisms that work together to filter, prioritize, and select a relevant subset of the incoming information.\n",
    "\n",
    "Psychologists and neuroscientists have extensively documented the capacity limits of human cognition, but we do not yet fully understand all the sources of capacity limits.  Change detection tasks (\"spot the difference\" between two images) and multiple object tracking tasks (keep track of moving targets among visually identical moving distractors) demonstrate severely limited awareness (e.g. around 1 object identity, and 3-4 object locations).  Building models can help us understand these limits.\n",
    "\n",
    "Cognitive processes that do not require the limited resources of attention can operate automatically.  Training over time can sometimes transfer an effortful and attention-demanding task to become automatic.  For literate and educated people, reading is one of the most highly trained activities that we perform, and it becomes automatic.  The Stroop task pits the automaticity of reading against the somewhat less trained task of naming colors.\n",
    "\n",
    "**Setup and Installation**"
   ],
   "id": "1278efb1f30b54cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "%pip install psyneulink\n",
    "%pip install stroop\n",
    "\n",
    "import psyneulink as pnl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "9aeed48ba8ebccfc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## A simplified Stroop Model - Linear Stroop\n",
    "\n",
    "In this section, we will study a simple linear network, without hidden units. There are two input layers correspond to color inputs and word inputs, and they map to an output layer.\n",
    "\n",
    "Each layer has two units, representing red and green. For example, [1, 0] for the color input layer means the stimulus has red color. [0, 1] for the word input means the word is green (but the color of the word is controlled by the color input). And the activity for the output layer can be viewed as the response. For example, [.8, .1] can be thought as having stronger \"red response\" (than \"green response\").\n",
    "\n",
    "Here, we set the strength of the \"word\" processing 1.5 higher than the \"color\" processing. This means that the word stimulus input will have a stronger effect on the response layer than the color stimulus input.\n"
   ],
   "id": "21a8822fb4aa31bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# set the strength of the two processing pathways\n",
    "strength_color_processing = 1\n",
    "strength_word_processing  = 1.5\n",
    "\n",
    "# input layers\n",
    "color_inp = pnl.TransferMechanism(\n",
    "    default_variable=[0, 0],\n",
    "    function=pnl.Linear(slope=strength_color_processing),\n",
    "    name=\"Color\"\n",
    ")\n",
    "word_inp = pnl.TransferMechanism(\n",
    "    default_variable=[0, 0],\n",
    "    function=pnl.Linear(slope=strength_word_processing),\n",
    "    name=\"Word\"\n",
    ")\n",
    "\n",
    "# output layer\n",
    "response = pnl.TransferMechanism(\n",
    "    default_variable=[0, 0],\n",
    "    function=pnl.Linear(slope=1),\n",
    "    name=\"Response\"\n",
    ")\n",
    "\n",
    "# Place mechanisms and projections in composition\n",
    "linear_stroop = pnl.Composition(name=\"Linear Stroop\")\n",
    "linear_stroop.add_linear_processing_pathway(pathway = [color_inp, response])\n",
    "linear_stroop.add_linear_processing_pathway(pathway = [word_inp, response])"
   ],
   "id": "d27519bb69f9c8fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "linear_stroop.show_graph(output_fmt = 'jupyter')",
   "id": "13fa48e1c8e79704"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's see the responses to some example stimuli:\n",
    "\n",
    "- <span style=\"color:green\">Red</span>\n",
    "- <span style=\"color:red\">Red</span>\n",
    "- <span style=\"color:red\">XXX</span>\n",
    "- <span style=\"color:green\">Green</span>\n",
    "- <span style=\"color:red\">Green</span>\n",
    "- <span style=\"color:grey\">Green</span>"
   ],
   "id": "b6792e9a0fc24458"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Exercise 1{exercise}\n",
    "\n",
    "Categorize the above stimuli into \"neutral\", \"congruent\" and \"incongruent/conflict\" categories"
   ],
   "id": "ed0bddd2703cb557"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Solution{solution}\n",
    "\n",
    "- <span style=\"color:green\">Red</span> - incongruent\n",
    "- <span style=\"color:red\">Red</span> - congruent\n",
    "- <span style=\"color:red\">XXX</span> - neutral\n",
    "- <span style=\"color:green\">Green</span> - congruent\n",
    "- <span style=\"color:red\">Green</span> - incongruent\n",
    "- <span style=\"color:grey\">Green</span> - neutral"
   ],
   "id": "8a58a0781e26c863"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define all stimuli\n",
    "red = [1, 0]\n",
    "green = [0, 1]\n",
    "null = [0, 0]\n",
    "\n",
    "g_r = {color_inp: green,  word_inp: red}\n",
    "r_r = {color_inp: red,  word_inp: red}\n",
    "r_n = {color_inp: red,  word_inp: null}\n",
    "g_g = {color_inp: green,  word_inp: green}\n",
    "r_g = {color_inp: red,  word_inp: green}\n",
    "n_g = {color_inp: null, word_inp: green}\n",
    "\n",
    "all_stimuli = [g_r, r_r, r_n, g_g, r_g, n_g]"
   ],
   "id": "7f0ea21869b17745"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# run the model for all conditions\n",
    "responses = []\n",
    "for i, stimuli in enumerate(all_stimuli):\n",
    "    response = linear_stroop.run(stimuli)\n",
    "    responses.append(response)\n",
    "    print(f'Condition: {all_stimuli[i]} \\t Response = {response}')"
   ],
   "id": "39b61ef262f9acb8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Exercise 2{exercise}\n",
    "\n",
    "Interpret the data above. How does the model \"respond\" to each stimulus? Why does it respond in this way?"
   ],
   "id": "f54d5610de8e2aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Solution{solution}\n",
    "\n",
    "- <span style=\"color:green\">Red</span> -> red\n",
    "- <span style=\"color:red\">Red</span> -> red\n",
    "- <span style=\"color:red\">XXX</span> -> red\n",
    "- <span style=\"color:green\">Green</span> -> green\n",
    "- <span style=\"color:red\">Green</span> -> green\n",
    "- <span style=\"color:grey\">Green</span> -> green\n",
    "\n",
    "It \"ignores\" the color of the word and always responds with the word. This is because the strength of the word processing is higher than the color processing."
   ],
   "id": "3b2dbbcf7894e4df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## A more complex Stroop Model - Adding the task demand unit\n",
    "\n",
    "The above model does not capture \"control\". It always responds with the word even if the \"task\" is to respond to the color of the word. Here, we will add a \"task demand\" unit that will add activity to the color processing pathway or to the word processing pathway.\n",
    "\n",
    "The weights and parameters are adapted from Cohen et al. (1990)."
   ],
   "id": "ac6e89ec85b49168"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Input units\n",
    "color_inp = pnl.TransferMechanism(name='Color Input', default_variable=[0, 0])\n",
    "word_inp = pnl.TransferMechanism(name='Word Input', default_variable=[0, 0])\n",
    "\n",
    "# Here, we have an additional task demand unit\n",
    "task_demand = pnl.TransferMechanism(name='Task Demand', default_variable=[0, 0])\n",
    "\n",
    "# We add a hidden layer in that will add the task demand activity to the color or word processing\n",
    "color_hidden = pnl.TransferMechanism(\n",
    "    default_variable=[0, 0], function=pnl.Logistic(gain=1., bias=-4.), name='Color hidden')\n",
    "word_hidden = pnl.TransferMechanism(\n",
    "    default_variable=[0, 0], function=pnl.Logistic(gain=1., bias=-4.), name='Word hidden')\n",
    "\n",
    "# We add a response layer, just like before (Here we use a different function to make the response units adapt  a value between 0 and 1)\n",
    "response = pnl.TransferMechanism(\n",
    "    default_variable=[0, 0], function=pnl.Logistic, name='Response')\n",
    "\n",
    "# We add projections\n",
    "\n",
    "# Input to hidden\n",
    "wts_clr_ih = pnl.MappingProjection(\n",
    "    matrix=[[2.2, -2.2], [-2.2, 2.2]], name='Color input to hidden')\n",
    "\n",
    "wts_wrd_ih = pnl.MappingProjection(\n",
    "    matrix=[[2.6, -2.6], [-2.6, 2.6]], name='Word input to hidden')\n",
    "\n",
    "# Task demand to hidden\n",
    "wts_tc = pnl.MappingProjection(\n",
    "    matrix=[[4.0, 4.0], [0, 0]], name='Color naming')\n",
    "\n",
    "wts_tw = pnl.MappingProjection(\n",
    "    matrix=[[0, 0], [4.0, 4.0]], name='Word reading')\n",
    "\n",
    "# Hidden to response\n",
    "wts_clr_r = pnl.MappingProjection(\n",
    "    matrix=[[1.3, -1.3], [-1.3, 1.3]], name='Color hidden to Response')\n",
    "wts_wrd_r = pnl.MappingProjection(\n",
    "    matrix=[[2.5, -2.5], [-2.5, 2.5]], name='Word hidden to Response')\n",
    "\n",
    "# build the model\n",
    "complex_stroop = pnl.Composition(name='Complex Stroop')\n",
    "\n",
    "# pathways\n",
    "complex_stroop.add_linear_processing_pathway([color_inp, wts_clr_ih, color_hidden])\n",
    "complex_stroop.add_linear_processing_pathway([word_inp, wts_wrd_ih, word_hidden])\n",
    "complex_stroop.add_linear_processing_pathway([task_demand, wts_tc, color_hidden])\n",
    "complex_stroop.add_linear_processing_pathway([task_demand, wts_tw, word_hidden])\n",
    "complex_stroop.add_linear_processing_pathway([color_hidden, wts_clr_r, response])\n",
    "complex_stroop.add_linear_processing_pathway([word_hidden, wts_wrd_r, response])"
   ],
   "id": "33fc766d8f400c60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "complex_stroop.show_graph(output_fmt = 'jupyter')",
   "id": "a6c755deb2349294"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Exercise 3{exercise}\n",
    "\n",
    "Make sure you understand the weights assigned to the matrices. Why are there negative weights? What do they represent?\n",
    "\n",
    "Hint:\n",
    "Try to interpret \"how\" activation is flowing. What is the sender-unit and what is the receiver-unit?\n",
    "\n"
   ],
   "id": "59c4060fb1a9318a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Solution{solution}\n",
    "\n",
    "Example: Color input to hidden:\n",
    "```\n",
    "wts_clr_ih = pnl.MappingProjection(\n",
    "    matrix=[[2.2, -2.2], [-2.2, 2.2]], name='Color input to hidden')\n",
    "```\n",
    "This matrix is responsible for the flow of activation from the color input to the color hidden unit. For example, if the color input is `[1, 0]` (red), the first unit (red) of the color hidden layer will receive an activation of `2.2`. However, the second unit (green) of the color hidden layer will be \"inhibited\" and receive an activation of `-2.2`.\n",
    "\n",
    "The negative weights are a sort of inhibition (not between units in the same layer, but between units in different layers). The input units not only \"activate\" their respective hidden units, but also \"inhibit\" the other hidden unit.\n",
    "\n"
   ],
   "id": "3c8c64397534fc29"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Exercise 4{exercise}\n",
    "\n",
    "The activation function of the hidden units is a logistic function with a bias of '-4'. Suspiciously, the task demand unit has a weight of '4' to the hidden units. Can you guess if this is a coincidence or if there is a reason behind this?"
   ],
   "id": "50def2914ebefa98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Hint{hint}\n",
    "\n",
    "Remember, the form of the logistic function:\n",
    "\n",
    "![logistic](log_fct.png)"
   ],
   "id": "ed810751475e5290"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Solution{solution}\n",
    "\n",
    "The bias of `-4` means that an activation between 0 and 1 will not activate the hidden unit by a lot. The logistic function is not very \"sensitive\" in the area between -4 and -3. However, adding exactly 4 to the input via the task demand unit will put the hidden unit into the sensitive area of the logistic function. In this sense the task demand unit \"modulates\" the behaviour of the hidden units by putting them into a sensitive area of the logistic function."
   ],
   "id": "a1d00424681cf039"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's see the responses to some example stimuli to see if the model is working as expected.",
   "id": "69f7ae3d0a137728"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "red = [1, 0]\n",
    "green = [0, 1]\n",
    "\n",
    "color_naming = [1, 0]\n",
    "word_naming = [0, 1]\n",
    "\n",
    "# Congruent stimuli for both color naming and word reading:\n",
    "con_col = {color_inp: red, word_inp: red, task_demand: color_naming}\n",
    "con_word = {color_inp: red, word_inp: red, task_demand: word_naming}\n",
    "\n",
    "# Incongruent stimuli for both color naming and word reading:\n",
    "inc_col = {color_inp: red, word_inp: green, task_demand: color_naming}\n",
    "inc_word = {color_inp: red, word_inp: green, task_demand: word_naming}\n",
    "\n",
    "all_stimuli = [con_col, con_word, inc_col, inc_word]"
   ],
   "id": "db8384d32dbae96a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Exercise 5{exercise}\n",
    "\n",
    "Before running the model, can you \"order\" the stimuli from the responses with the expected highest activation to the lowest activation?"
   ],
   "id": "f0d9eb5451b9c5d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# run the model for all conditions\n",
    "for stimuli in all_stimuli:\n",
    "    response = complex_stroop.run(stimuli)\n",
    "    print(f'Condition: {stimuli} \\t Response = {response}')\n"
   ],
   "id": "e6c1d310ddd827be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The full stroop model\n",
    "\n",
    "Here's a qualitative replication of the original stroop model. Compared to the previous simplification (the model without recurrence), this network has explicit mechanism (on top of the output layer) for integrating information (should I make red response or green response?) over time to make a response. This is achieved by the <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/11488378\">leaky competing accumulator</a>.\n",
    "\n",
    "The important point here is that now the model has temporal dynamics, and it can be used to model reaction time very naturally. In comparison, for the previous model, we had to hypothesize the relation between output activity (of red vs. green) and reaction time."
   ],
   "id": "1f7dbc354c16ce30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Input units\n",
    "color_inp = pnl.TransferMechanism(name='Color Input', default_variable=[0, 0])\n",
    "word_inp = pnl.TransferMechanism(name='Word Input', default_variable=[0, 0])\n",
    "task_demand = pnl.TransferMechanism(name='Task Demand', default_variable=[0, 0])\n",
    "\n",
    "# Here, we integrate the activity instead of just passing it through\n",
    "INTEGRATION_RATE = .2\n",
    "UNIT_NOISE_STD = .01\n",
    "DEC_NOISE_STD = .1\n",
    "LEAK = 0\n",
    "COMPETITION = 1\n",
    "\n",
    "color_hidden = pnl.TransferMechanism(\n",
    "    default_variable=[0, 0],\n",
    "    function=pnl.Logistic(gain=1., bias=-4.),\n",
    "    integrator_mode=True,\n",
    "    integration_rate=INTEGRATION_RATE,\n",
    "    noise=pnl.NormalDist(standard_deviation=UNIT_NOISE_STD).function,\n",
    "    name='Color hidden')\n",
    "\n",
    "word_hidden = pnl.TransferMechanism(\n",
    "    default_variable=[0, 0],\n",
    "    function=pnl.Logistic(gain=1., bias=-4.),\n",
    "    integrator_mode=True,\n",
    "    integration_rate=INTEGRATION_RATE,\n",
    "    noise=pnl.NormalDist(standard_deviation=UNIT_NOISE_STD).function,\n",
    "    name='Word hidden')\n",
    "\n",
    "# The same is true for the output layer (for clarity, we use a different name than response)\n",
    "\n",
    "output = pnl.TransferMechanism(\n",
    "    default_variable=[0, 0],\n",
    "    function=pnl.Logistic,\n",
    "    integrator_mode=True,\n",
    "    integration_rate=INTEGRATION_RATE,\n",
    "    noise=pnl.NormalDist(standard_deviation=UNIT_NOISE_STD).function,\n",
    "    name='Output')\n",
    "\n",
    "# In addition, we have a decision layer, which is implemented as leaky competing accumulator\n",
    "decision = pnl.LCAMechanism(\n",
    "    default_variable=[0, 0],\n",
    "    leak=LEAK,\n",
    "    competition=COMPETITION,\n",
    "    noise=pnl.UniformToNormalDist(standard_deviation=DEC_NOISE_STD).function,\n",
    "    name='Decision'\n",
    ")\n",
    "\n",
    "# We add the same projections as before\n",
    "\n",
    "# Input to hidden\n",
    "wts_clr_ih = pnl.MappingProjection(\n",
    "    matrix=[[2.2, -2.2], [-2.2, 2.2]], name='Color input to hidden')\n",
    "\n",
    "wts_wrd_ih = pnl.MappingProjection(\n",
    "    matrix=[[2.6, -2.6], [-2.6, 2.6]], name='Word input to hidden')\n",
    "\n",
    "# Task demand to hidden\n",
    "wts_tc = pnl.MappingProjection(\n",
    "    matrix=[[4.0, 4.0], [0, 0]], name='Color naming')\n",
    "\n",
    "wts_tw = pnl.MappingProjection(\n",
    "    matrix=[[0, 0], [4.0, 4.0]], name='Word reading')\n",
    "\n",
    "# Hidden to response\n",
    "wts_clr_r = pnl.MappingProjection(\n",
    "    matrix=[[1.3, -1.3], [-1.3, 1.3]], name='Color hidden to Output')\n",
    "wts_wrd_r = pnl.MappingProjection(\n",
    "    matrix=[[2.5, -2.5], [-2.5, 2.5]], name='Word hidden to Output')\n",
    "\n",
    "# build the model\n",
    "full_stroop = pnl.Composition(name='Complex Stroop')\n",
    "\n",
    "# pathways\n",
    "full_stroop.add_linear_processing_pathway([color_inp, wts_clr_ih, color_hidden])\n",
    "full_stroop.add_linear_processing_pathway([word_inp, wts_wrd_ih, word_hidden])\n",
    "full_stroop.add_linear_processing_pathway([task_demand, wts_tc, color_hidden])\n",
    "full_stroop.add_linear_processing_pathway([task_demand, wts_tw, word_hidden])\n",
    "full_stroop.add_linear_processing_pathway([color_hidden, wts_clr_r, output])\n",
    "full_stroop.add_linear_processing_pathway([word_hidden, wts_wrd_r, output])\n",
    "full_stroop.add_linear_processing_pathway([output, pnl.IDENTITY_MATRIX, decision])"
   ],
   "id": "201b519d9158df79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "full_stroop.show_graph(output_fmt = 'jupyter')",
   "id": "117ca56ff4d14fdb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Exercise 6{exercise}\n",
    "\n",
    "What is the \"advantage\" of using an integrator model in general? Can you think about manipulations that can hardly be implemented in a non-integrator model?"
   ],
   "id": "5fc90cf7ee37b139"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Solution{solution}\n",
    "\n",
    "The integrator model can be used to model temporal dynamics. This is valuable for reaction times, but also to test manipulations like different SOAs (stimulus-onset-asynchrony). For example, the word can be shown before it is colored or vice versa. There can also be a delay or \"blanks\" between the word and color. These manipulations can be implemented in an integrator model, but not in a non-integrator model."
   ],
   "id": "4e220fa6dd37b336"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For this lab, a former TA wrote a python library to help with constructing stroop stimuli, which is imported below.\n",
    "- You don't need to understand what it does internally but if you want to see the internal, click <a href=\"https://github.com/qihongl/stroop-stimuli/blob/master/stroop/stimulus.py\">here</a>."
   ],
   "id": "dca49c96c5525c48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from stroop.stimulus import get_stimulus, TASKS, COLORS, CONDITIONS\n",
    "\n",
    "# calculate experiment metadata\n",
    "n_conditions = len(CONDITIONS)\n",
    "n_tasks = len(TASKS)\n",
    "n_colors = len(COLORS)\n",
    "\n",
    "# constants\n",
    "experiment_info = f\"\"\"\n",
    "stroop experiment info\n",
    "- {n_colors} colors:\\t {COLORS}\n",
    "- {n_colors} words:\\t {COLORS}\n",
    "- {n_tasks} tasks:\\t {TASKS}\n",
    "- {n_conditions} conditions:\\t {CONDITIONS}\n",
    "\"\"\"\n",
    "print(experiment_info)"
   ],
   "id": "8beba5a79fe4eba3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Define the inputs\n",
    "\n",
    "i.e. all CONDITIONS x TASKS for the experiment"
   ],
   "id": "231f15352619e2d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# the length of the stimulus sequence\n",
    "n_time_steps = 120\n",
    "\n",
    "# color naming - cong\n",
    "inputs_cn_con = get_stimulus(\n",
    "    color_inp, 'red', word_inp, 'red', task_demand, 'color naming', n_time_steps\n",
    ")\n",
    "# color naming - incong\n",
    "inputs_cn_cfl = get_stimulus(\n",
    "    color_inp, 'red', word_inp, 'green', task_demand, 'color naming', n_time_steps\n",
    ")\n",
    "# color naming - control\n",
    "inputs_cn_ctr = get_stimulus(\n",
    "    color_inp, 'red', word_inp, None, task_demand, 'color naming', n_time_steps\n",
    ")\n",
    "# word reading - cong\n",
    "inputs_wr_con = get_stimulus(\n",
    "    color_inp, 'red', word_inp, 'red', task_demand, 'word reading', n_time_steps\n",
    ")\n",
    "# word reading - incong\n",
    "inputs_wr_cfl = get_stimulus(\n",
    "    color_inp, 'green', word_inp, 'red', task_demand, 'word reading', n_time_steps\n",
    ")\n",
    "# word reading - control\n",
    "inputs_wr_ctr = get_stimulus(\n",
    "    color_inp, None, word_inp, 'red', task_demand, 'word reading', n_time_steps\n",
    ")"
   ],
   "id": "1718aef38e87e1a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Visualize an input stimulus, note that the stimulus here is a sequence",
   "id": "85601890bb5f9cbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# choose the condition you want to visualize\n",
    "stimuli_list_plt = inputs_cn_cfl\n",
    "\n",
    "# set the title and the nodes to plot\n",
    "titles_plt = ['color input', 'word input', 'task instruction']\n",
    "input_nodes_plt = [color_inp, word_inp]\n",
    "\n",
    "# plot the data\n",
    "f, axes = plt.subplots(3, 1, figsize=(9, 8), sharex=True)\n",
    "for i, node_i in enumerate(input_nodes_plt):\n",
    "    for j in range(n_colors):\n",
    "        axes[i].plot(stimuli_list_plt[node_i][:, j])\n",
    "    axes[i].legend(['red', 'green'], frameon=False)\n",
    "\n",
    "axes[2].plot(stimuli_list_plt[task_demand])\n",
    "axes[2].legend(['color naming', 'word reading'], frameon=False)\n",
    "\n",
    "# mark the plot\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_ylabel('Input')\n",
    "    ax.set_title(titles_plt[i])\n",
    "axes[2].set_xlabel('Time')\n",
    "axes[0].legend(['red', 'green'], frameon=False)\n",
    "\n",
    "f.tight_layout()\n"
   ],
   "id": "e43b09e2c151fc8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Exercise 7{exercise}\n",
    "\n",
    "Can you explain the plot above? What does each line present? What is the x-axis and y-axis?"
   ],
   "id": "478d43cc9d75bee4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Solution{solution}\n",
    "\n",
    "Since we are using an integrator model, we will use a sequence of inputs. The x-axis is the \"time\" or index of the input, the y-value is the input value for each of the units."
   ],
   "id": "9daec806f1a43d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We run the model on a single stimulus to see how the activation evolves over time.",
   "id": "3819341449584c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# run the model on one stimulus\n",
    "inputs = inputs_cn_cfl\n",
    "full_stroop.run(\n",
    "    context=999,\n",
    "    inputs=inputs,\n",
    "    num_trials=n_time_steps,\n",
    ")\n",
    "activation = full_stroop.results"
   ],
   "id": "acd089c5777d51f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "f, ax = plt.subplots(1,1, figsize=(9,5))\n",
    "for i in range(n_colors):\n",
    "    ax.plot(np.squeeze(activation)[:,i])\n",
    "\n",
    "ax.set_title('Decision activity over time')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Activity')\n",
    "f.legend(['%s unit' % c for c in COLORS], frameon=False, bbox_to_anchor=(1.1,.7))\n",
    "f.tight_layout()"
   ],
   "id": "79027c90dcaaf4c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's now run the model on various conditions and see how the model performs.",
   "id": "998e1a559b4bf998"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_model(execution_id, n_repeats, inputs, n_time_steps=100):\n",
    "    \"\"\"define how to run the model\"\"\"\n",
    "\n",
    "    acts = np.zeros((n_repeats, n_time_steps, 2))\n",
    "    for i in range(n_repeats):\n",
    "        print(f'{execution_id}', end=' ')\n",
    "        full_stroop.run(\n",
    "            context=execution_id,\n",
    "            inputs=inputs,\n",
    "            num_trials=n_time_steps,\n",
    "        )\n",
    "        execution_id += 1\n",
    "        # log acts\n",
    "        acts[i, :, :] = np.squeeze(full_stroop.results)\n",
    "    return acts, execution_id"
   ],
   "id": "696c65aa0f8a4d57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "execution_id = 100\n",
    "n_repeats = 1\n",
    "\n",
    "# combine the task stimuli\n",
    "cn_input_list = [inputs_cn_ctr, inputs_cn_cfl, inputs_cn_con]\n",
    "wr_input_list = [inputs_wr_ctr, inputs_wr_cfl, inputs_wr_con]\n",
    "\n",
    "# preallocate variables to hold activity\n",
    "A_cn = {condition: None for condition in CONDITIONS}\n",
    "A_wr = {condition: None for condition in CONDITIONS}\n",
    "\n",
    "# run all conditions, color naming\n",
    "for i, condition in enumerate(CONDITIONS):\n",
    "    print(f'\\nRunning color naming, condition = {condition}')\n",
    "    A_cn[condition], execution_id = run_model(execution_id,\n",
    "        n_repeats, cn_input_list[i]\n",
    "    )\n",
    "# run all conditions, word reading\n",
    "for i, condition in enumerate(CONDITIONS):\n",
    "    print(f'\\nRunning word reading, condition = {condition}')\n",
    "    print(f'Execution ids:', end=' ')\n",
    "    A_wr[condition], execution_id = run_model(execution_id,\n",
    "        n_repeats, wr_input_list[i]\n",
    "    )\n",
    "print('Done!')\n"
   ],
   "id": "38e6b9915289ade0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here, we compute the Reaction Time (RT) for each condition. The RT is defined as the time when the activity of the decision layer exceeds a certain threshold (0.9).",
   "id": "de6406b44a8b5b1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_rt(act, threshold=.9):\n",
    "    \"\"\"compute reaction time\n",
    "    take the activity of the decision layer...\n",
    "    check the earliest time point when activity > threshold...\n",
    "    call that RT\n",
    "    *RT=np.nan if timeout\n",
    "    \"\"\"\n",
    "    n_time_steps_, N_UNITS_ = np.shape(act)\n",
    "    rts = np.full(shape=(N_UNITS_,), fill_value=np.nan)\n",
    "    for i in range(N_UNITS_):\n",
    "        tps_pass_threshold = np.where(act[:, i] > threshold)[0]\n",
    "        if len(tps_pass_threshold) > 0:\n",
    "            rts[i] = tps_pass_threshold[0]\n",
    "    return np.nanmin(rts)"
   ],
   "id": "dcdc304c2fc0c17b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# compute RTs for color naming and word reading\n",
    "threshold = .9\n",
    "RTs_cn = {condition: None for condition in CONDITIONS}\n",
    "RTs_wr = {condition: None for condition in CONDITIONS}\n",
    "for i, condition in enumerate(CONDITIONS):\n",
    "    RTs_cn[condition] = np.array(\n",
    "        [compute_rt(A_cn[condition][i, :, :], threshold) for i in range(n_repeats)]\n",
    "    )\n",
    "    RTs_wr[condition] = np.array(\n",
    "        [compute_rt(A_wr[condition][i, :, :], threshold) for i in range(n_repeats)]\n",
    "    )\n",
    "\n",
    "# organize data for plotting, color naming and word reading\n",
    "mean_rt_cn = [np.nanmean(RTs_cn[condition]) for condition in CONDITIONS]\n",
    "mean_rt_wr = [np.nanmean(RTs_wr[condition]) for condition in CONDITIONS]\n",
    "std_rt_cn = [np.nanstd(RTs_cn[condition]) for condition in CONDITIONS]\n",
    "std_rt_wr = [np.nanstd(RTs_wr[condition]) for condition in CONDITIONS]\n",
    "xtick_vals = range(len(CONDITIONS))"
   ],
   "id": "e6fd71384dbff6ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here, we recreate Figure 5 from Cohen et al. (1990).",
   "id": "5b2d4da362139e04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# plot RT\n",
    "f, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.errorbar(\n",
    "    x=xtick_vals, y=mean_rt_cn, yerr=std_rt_cn,\n",
    "    label='color naming', color='black'\n",
    ")\n",
    "ax.errorbar(\n",
    "    x=xtick_vals, y=mean_rt_wr, yerr=std_rt_wr,\n",
    "    label='word reading', color='black', linestyle='--',\n",
    ")\n",
    "ax.set_ylabel('Reaction time (n cycles)')\n",
    "ax.set_xticks(xtick_vals)\n",
    "ax.set_xticklabels(CONDITIONS)\n",
    "ax.set_xlabel('Condition')\n",
    "ax.set_title('RT under various conditions')\n",
    "f.legend(frameon=False, bbox_to_anchor=(1, .9))\n",
    "f.tight_layout()"
   ],
   "id": "d16ff6d93044cb53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Exercise 8{exercise}\n",
    "\n",
    "Create a similar model for 4 instead of 2 colors."
   ],
   "id": "112a47b5b7476b9d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
