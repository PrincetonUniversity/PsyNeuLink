{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Quickstart Tutorial\n",
    "\n",
    "The examples below are intended to provide quick illustrations of some of PsyNeuLink's basic and more advanced\n",
    "capabilities. They assume some experience with computational modeling and/or relevant background knowledge.\n",
    "\n",
    "\n",
    "**Installation & Setup**\n"
   ],
   "id": "838f2aec617a1a5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "!pip install psyneulink"
   ],
   "id": "3269c517efcba28d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import psyneulink as pnl",
   "id": "e1aeedcc6280ead0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Simple Configurations\n",
    "\n",
    "Mechanisms can be executed on their own (to gain familiarity with their operation, or for use in other Python\n",
    "applications), or linked together and run in a Composition to implement part of, or an entire model. Linking\n",
    "Mechanisms for execution can be as simple as creating them and then assiging them to a Composition in a list --\n",
    "PsyNeuLink provides the necessary Projections that connect each to the next one in the list, making reasonable\n",
    "assumptions about their connectivity.  The following example creates a 3-layered 5-2-5 neural network\n",
    "encoder network, the first layer of which takes an an array of length 5 as its input, and uses a `Linear` function\n",
    "(the default for a `ProcessingMechanism`), and the other two of which take 1d arrays of the specified sizes and use a\n",
    "`Logistic` function:"
   ],
   "id": "6caf9e7bdd12fadc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Construct the Mechanisms:\n",
    "input_layer = pnl.ProcessingMechanism(input_shapes=5, name='Input')\n",
    "hidden_layer = pnl.ProcessingMechanism(input_shapes=2, function=pnl.Logistic, name='hidden')\n",
    "output_layer = pnl.ProcessingMechanism(input_shapes=5, function=pnl.Logistic, name='output')\n",
    "\n",
    "# Construct the Composition:\n",
    "my_encoder = pnl.Composition(pathways=[[input_layer, hidden_layer, output_layer]])"
   ],
   "id": "abfc073b59415ed4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Each of the Mechanisms can be executed individually, by simply calling its `execute <Mechanism_Base.execute>` method\n",
    "with an appropriately-sized input array, for example:"
   ],
   "id": "f29f61b77c2e3f9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "output_layer.execute([0, 2.5, 10.9, 2, 7.6])",
   "id": "f12285131ca6aba7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "The Composition connects the Mechanisms into a pathway that form a graph, which can be shown using its `show_graph\n",
    "<ShowGraph.show_graph>` method:\n",
    "\n",
    ".. _BasicsAndPrimer_Simple_Pathway_Example_Figure:\n",
    "\n",
    ".. figure:: _static/BasicsAndPrimer_SimplePathway_fig.svg\n",
    "   :width: 30%\n",
    "\n",
    "   **Composition Graph.**  Representation of the graph of the simple Composition in the example above.  Note that the\n",
    "   Input Mechanism for the Composition is colored green (to designate it is an `INPUT` node), and its output\n",
    "   Mechanism is colored Red (to designate it at a `OUTPUT` node).\n",
    "\n",
    "As the name of the ``show_graph()`` method suggests, Compositions are represented in PsyNeuLink as graphs, using a\n",
    "standard dependency dictionary format, so that they can also be submitted to other graph theoretic packages for\n",
    "display and/or analysis (such as `NetworkX <https://networkx.github.io>`_ and `igraph <http://igraph.org/redirect\n",
    ".html>`_).  They can also be exported as a JSON file, in a format that is currently being developed for the exchange\n",
    "of computational models in neuroscience and psychology (see `json`)\n",
    "\n",
    ".. XXX USE show_graph(show_node_structure=True) HERE OR ABOVE::\n",
    "\n",
    "The Composition can be run by calling its `run <Composition.run>` method, with an input array appropriately sized for\n",
    "the first Mechanism in the pathway (in this case, the input_layer)::\n",
    "\n",
    "    my_encoder.run([1, 4.7, 3.2, 6, 2])\n",
    "    [array([0.88079707, 0.88079707, 0.88079707, 0.88079707, 0.88079707])]\n",
    "\n",
    "The order in which Mechanisms appear in the list of the **pathways** argument of the Composition's constructor\n",
    "determines their order in the pathway.  More complicated arrangements can be created by using one of the Compositions\n",
    "`pathway addition methods <Composition_Pathway_Addition_Methods>`, by adding nodes individually using a\n",
    "Composition's `add_nodes <Composition.add_nodes>` method, and/or by creating intersecting pathways, as shown in some\n",
    "of the examples further below.\n",
    "\n",
    "PsyNeuLink picks sensible defaults when necessary Components are not specified.  In the example above no `Projections\n",
    "<Projection>` were actually specified, so PsyNeuLink automatically created the appropriate types (in this case,\n",
    "`MappingProjections<MappingProjection>`), and sized them appropriately to connect each pair of Mechanisms. Each\n",
    "Projection has a `matrix <Projection_Base.matrix>` parameter that weights the connections between the elements of the\n",
    "output\n",
    "of its `sender <Projection.sender>` and those of the input to its `receiver <Projection.receiver>`.  Here, the\n",
    "default is to use a `FULL_CONNECTIVITY_MATRIX`, that connects every element of the sender's array to every element of\n",
    "the receiver's array with a weight of 1. However, it is easy to specify a Projection explicitly, including its\n",
    "matrix, simply by inserting them in between the Mechanisms in the pathway::\n",
    "\n",
    "    my_projection = MappingProjection(matrix=(.2 * np.random.rand(2, 5)) - .1))\n",
    "    my_encoder = Composition()\n",
    "    my_encoder.add_linear_processing_pathway([input_layer, my_projection, hidden_layer, output_layer])\n",
    "\n",
    "The first line above creates a Projection with a 2x5 matrix of random weights constrained to be between -.1 and +.1,\n",
    "which is then inserted in the pathway between the ``input_layer`` and ``hiddeen_layer``.  Note that here, one of the\n",
    "Composition's `pathway addition methods <Composition_Pathway_Addition_Methods>` is used to create the pathway, as an\n",
    "alternative to specifying it in the **pathways** argument of the constructor (as shown in the initial example). The\n",
    "Projection's matrix itself could also have been inserted directly, as follows::\n",
    "\n",
    "    my_encoder.add_linear_processing_pathway([input_layer, (.2 * np.random.rand(2, 5)) - .1)), hidden_layer, output_layer])\n",
    "\n",
    "PsyNeuLink knows to create a MappingProjection using the matrix.  PsyNeuLink is also flexible.  For example,\n",
    "a recurrent Projection from the ``output_layer`` back to the ``hidden_layer`` can be added simply by adding another\n",
    "entry to the pathway::\n",
    "\n",
    "    my_encoder.add_linear_processing_pathway([input_layer, hidden_layer, output_layer, hidden_layer])\n",
    "\n",
    "This tells PsyNeuLink to create a Projection from the output_layer back to the hidden_layer.  The same could have also\n",
    "been accomplished by explicitly creating the recurrent connection::\n",
    "\n",
    "    my_encoder.add_linear_processing_pathway([input_layer, hidden_layer, output_layer])\n",
    "    recurent_projection = MappingProjection(sender=output_layer,\n",
    "                      receiver=hidden_layer)\n",
    "    my_encoder.add_projection(recurent_projection)\n",
    "\n",
    "\n",
    ".. _BasicsAndPrimer_Elaborate_Configurations:\n",
    "\n",
    "More Elaborate Configurations\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Configuring more complex models is also straightforward.  For example, the script below implements a model of the\n",
    "`Stroop task <https://en.wikipedia.org/wiki/Stroop_effect>`_ by creating two feedforward neural network pathways\n",
    "-- one for color naming and another for word reading -- as well as a corresponding pair of pathways that determine which\n",
    "of those to perform based on a task instruction. These all converge on a common output mechanism that projects to a\n",
    "drift diffusion (DDM) decision mechanism responsible for determining the response::\n",
    "\n",
    "    # Construct the color naming pathway:\n",
    "    color_input = ProcessingMechanism(name='COLOR INPUT', input_shapes=2) # note: default function is Linear\n",
    "    color_input_to_hidden_wts = np.array([[2, -2], [-2, 2]])\n",
    "    color_hidden = ProcessingMechanism(name='COLOR HIDDEN', input_shapes=2, function=Logistic(bias=-4))\n",
    "    color_hidden_to_output_wts = np.array([[2, -2], [-2, 2]])\n",
    "    output = ProcessingMechanism(name='OUTPUT', input_shapes=2 , function=Logistic)\n",
    "    color_pathway = [color_input, color_input_to_hidden_wts, color_hidden, color_hidden_to_output_wts, output]\n",
    "\n",
    "    # Construct the word reading pathway (using the same output_layer)\n",
    "    word_input = ProcessingMechanism(name='WORD INPUT', input_shapes=2)\n",
    "    word_input_to_hidden_wts = np.array([[3, -3], [-3, 3]])\n",
    "    word_hidden = ProcessingMechanism(name='WORD HIDDEN', input_shapes=2, function=Logistic(bias=-4))\n",
    "    word_hidden_to_output_wts = np.array([[3, -3], [-3, 3]])\n",
    "    word_pathway = [word_input, word_input_to_hidden_wts, word_hidden, word_hidden_to_output_wts, output]\n",
    "\n",
    "    # Construct the task specification pathways\n",
    "    task_input = ProcessingMechanism(name='TASK INPUT', input_shapes=2)\n",
    "    task_color_wts = np.array([[4,4],[0,0]])\n",
    "    task_word_wts = np.array([[0,0],[4,4]])\n",
    "    task_color_pathway = [task_input, task_color_wts, color_hidden]\n",
    "    task_word_pathway = [task_input, task_word_wts, word_hidden]\n",
    "\n",
    "    # Construct the decision pathway:\n",
    "    decision = DDM(name='DECISION', input_format=ARRAY)\n",
    "    decision_pathway = [output, decision]\n",
    "\n",
    "    # Construct the Composition:\n",
    "    Stroop_model = Composition(name='Stroop Model')\n",
    "    Stroop_model.add_linear_processing_pathway(color_pathway)\n",
    "    Stroop_model.add_linear_processing_pathway(word_pathway)\n",
    "    Stroop_model.add_linear_processing_pathway(task_color_pathway)\n",
    "    Stroop_model.add_linear_processing_pathway(task_word_pathway)\n",
    "    Stroop_model.add_linear_processing_pathway(decision_pathway)\n",
    "\n",
    "This is a simplified version the model described in `Cohen et al. (1990) <https://www.researchgate\n",
    ".net/publication/20956134_Cohen_JD_McClelland_JL_Dunbar_K_On_the_control_of_automatic_processes_a_parallel_distributed_processing_account_of_the_Stroop_effect_Psychol_Rev_97_332-361>`_\n",
    ".\n",
    "\n",
    ".. FIX: ADD LINK TO STROOP MODEL\n",
    ".. a more complete version of which can be found in the\n",
    ".. `PsyNeuLink Library <https://princetonuniversity.github.io/PsyNeuLink/Library.html>`_ at `Stroop Model <XXXX GET FROM Q>`.\n",
    "\n",
    "The figure belows shows the model using the Composition's `show_graph <ShowGraph.show_graph>` method.\n",
    "\n",
    ".. _BasicsAndPrimer_Simple_Stroop_Example_Figure:\n",
    "\n",
    ".. figure:: _static/BasicsAndPrimer_Stroop_Model.svg\n",
    "   :width: 100%\n",
    "\n",
    "   **Stroop Model.** Representation of the Composition in the example above.\n",
    "\n",
    "Running the model is as simple as generating some inputs and then providing them to the `run <Composition.run>`\n",
    "method.  Inputs are specified in a dictionary, with one entry for each of the Composition's `INPUT`\n",
    "Mechanisms;  each entry contains a list of the inputs for the specified Mechanism, one for each trial to be run.\n",
    "The following defines two stimuli to use as the color and word inputs (``red`` and ``green``), and two for use as the\n",
    "task input (``color`` and ``word``), and then uses them to run the model for a color naming congruent trial, followed\n",
    "by a color naming incongruent trial::\n",
    "\n",
    "    red =   [1,0]\n",
    "    green = [0,1]\n",
    "    word =  [0,1]\n",
    "    color = [1,0]\n",
    "                                       # Trial 1  Trial 2\n",
    "    Stroop_model.run(inputs={color_input:[red,     red   ],\n",
    "                             word_input: [red,     green ],\n",
    "                             task_input: [color,   color ]})\n",
    "    print(Stroop_model.results)\n",
    "    >> [[array([1.]), array([2.80488344])], [array([1.]), array([3.94471513])]]\n",
    "\n",
    "When a Composition is run, its `results <Composition.results>` attribute stores the values of its `OUTPUT` Mechanisms\n",
    "at the end of each `trial <TimeScale.TRIAL>`. In this case, the `DDM` Mechanism is the only `OUTPUT` Mechanism, and it\n",
    "has two output values by default: the outcome of the decision (1 or -1, in this case corresponding to ``red`` or\n",
    "``green``), and the estimated mean decision time for the decision (in seconds).  So, the value returned by the `results\n",
    "<Composition.results>` attribute is a 3d array containing two 2d arrays, each of which has the two outputs of the DDM\n",
    "for each `trial <TimeScale.TRIAL>` (notice that the estimated response time for the second, incongruent trial was\n",
    "significantly longer than for the first, congruent trial;  note also that, on some executions it might return -1 as\n",
    "the response in the second trials since, by default, the `function <DDM.function>` used for the decision process has\n",
    "a non-zero `noise <DriftDiffusionAnalytical.noise>` term).\n",
    "\n",
    ".. _BasicsAndPrimer_Dynamics_of_Execution:\n",
    "\n",
    "Dynamics of Execution\n",
    "~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    ".. XXX\n",
    ".. - Execute at multiple times scales:\n",
    "..   • run DDM in integrator mode\n",
    "..   • but notice that it only executes one step of integration\n",
    "..   • so, can apply condition that causes it to execute until it \"completes\" which, for a DDM is when the process\n",
    "..     the value specified in its threhosld parameter, as follows::\n",
    "\n",
    "One of the most powerful features of PsyNeuLink is its ability to simulate models with Components that execute at\n",
    "different time scales.  By default, each Mechanism executes once per pass through the Composition, in the order\n",
    "determined by the projections between them (and shown in the `show_graph <ShowGraph.show_graph>` method.  In the\n",
    "``Stroop_model`` above, the ``decision`` Mechanism executes once per pass, just after the ``ouput`` Mechanism.  The\n",
    "``decision`` Mechanism is a `DDM`.  This uses `DriftDiffusionAnalytical` as its default `function <DDM.function>`,\n",
    "which computes an analytic solution to the distribution of responses using the DDM integration process, and returns\n",
    "both the probability of crossing a specified `threshold <DriftDiffusionAnalytical.threshold>`), and the mean\n",
    "crossing time.  However, it is also possible to simulate the dynamics of the integration process.  This can be done by\n",
    "assigning `DriftDiffusionIntegrator` as the Mechanism's `function <DDM.function>` and, in the call to the Composition's\n",
    "`run <Composition.run>` method, specifying that a `trial <TimeScale.TRIAL>` terminates only when the ``decision``\n",
    "Mechanism has completed its execution, as follows::\n",
    "\n",
    "    # Modify consruction of decision Mechanism:\n",
    "    decision = DDM(name='DECISION',\n",
    "                   input_format=ARRAY,\n",
    "                   reset_stateful_function_when=AtTrialStart(),\n",
    "                   function=DriftDiffusionIntegrator(noise=0.5, threshold=20)\n",
    "                   )\n",
    "    Stroop_model.run(inputs={color_input:red, word_input:green, task_input:color},\n",
    "                     termination_processing={TimeScale.TRIAL: WhenFinished(decision)}\n",
    "                     )\n",
    "    print (Stroop_model.results)\n",
    "    >> [[array([[20.]]), array([[126.]])]]\n",
    "\n",
    "The output is now the result of the `DriftDiffusionIntegrator`, which is the value of the decision variable when it\n",
    "crosses threshold (which is, by definition, equal to either the postive or negative value of the `threshold\n",
    "<DriftDiffusionAnalytical.threshold>` attribute), and the number of executions it took to do so.  Since the ``decision``\n",
    "Mechanism is the last (`TERMINAL`) Mechanism of the Composition, it is also its `OUTPUT` Mechanism.  Therefore, its\n",
    "output is recorded in the `results <Composition.results>` attribute of the Stroop model, as shown above (note: because\n",
    "there is noise in the integration process, running the model several times produces varying response times).\n",
    "\n",
    "This version of the model includes Mechanisms that execute over different time-scales. The ProcessingMechanisms\n",
    "completed their computations in a single execution, whereas the DDM took many executions to complete its computation.\n",
    "In this case, the coordination of time scales was straightforward, since the DDM was the last Mechanism in the\n",
    "Composition:  the ProcessingMechanisms in each pathway executed in sequence, ending in the DDM which executed until\n",
    "it was complete.  PsyNeuLink's `Scheduler` can be used to implement more complicated dependencies among Mechanisms, by\n",
    "creating one or more `Conditions <Condition>` for execution of those Mechanisms and assigning those to the Composition's\n",
    "`Scheduler`. Conditions can specify the behavior of a Mechanism on its own (e.g., how many times it should be executed\n",
    "in each `trial <TimeScale.TRIAL>`), its behavior relative to one or more other Components (e.g., how many times it\n",
    "should wait for another Mechanism to execute before it does so), or even arbitrary functions (e.g., a convergence\n",
    "criterion for the settling of a recurrent network). For example, the following implements a version of the model above\n",
    "that uses a `leaky competing accumulator <https://www.ncbi.nlm.nih.gov/pubmed/11488378>`_ (`LCAMechanism`) for the\n",
    "``task`` Mechanism.  The latter settles for a specified number of executions before the color and word hidden layers\n",
    "execute, simulating a situation in which the task instruction is processed before processing the color or word stimuli::\n",
    "\n",
    "    # Modify consruction of task Mechanism:\n",
    "    task = LCAMechanism(name='TASK', input_shapes=2)\n",
    "\n",
    "    # Assign conditions to scheduler:\n",
    "    Stroop_model.scheduler.add_condition(color_hidden, EveryNExecutions(task, 10))\n",
    "    Stroop_model.scheduler.add_condition(word_hidden, EveryNExecutions(task, 10))\n",
    "\n",
    "    # Run with scheduler:\n",
    "    Stroop_model.run(inputs={color_input:red, word_input:green, task_input:color})\n",
    "    print (Stroop_model.results)\n",
    "    >>[[array([[20.]]), array([[42.]])]]\n",
    "\n",
    "In the example above, the ``color_hidden`` and ``word_hidden`` Mechanisms both wait to execute until the ``task``\n",
    "Mechanism has executed 100 times.  They could also each have been made to wait different numbers of times;  in that\n",
    "case, since the ``output`` Mechanism depends on both them, it would have waited until they had both executed before\n",
    "doing so itself.  This example also imposes a fixed \"setting time\" (100 executions) on the ``task`` Mechanism. However,\n",
    "it could also be allowed to settle until it reaches some criterion.  For example, the ``color_hidden`` and\n",
    "``word_hidden`` can be configured to wait until the value of the ``task`` Mechanism \"converges\", by changing the\n",
    "conditions for execution of the ``color_hidden`` and ``task_hidden`` Mechanism's to depend on a function, as follows::\n",
    "\n",
    "    # Define a function that detects when the a Mechanism's value has converged, such that the change in all of the\n",
    "    elements of its value attribute from the last execution (given by its delta attribute) falls below ``epsilon``\n",
    "\n",
    "    def converge(mech, thresh):\n",
    "        return all(abs(v) <= thresh for v in mech.delta)\n",
    "\n",
    "    # Add Conditions to the ``color_hidden`` and ``word_hidden`` Mechanisms that depend on the converge function:\n",
    "    epsilon = 0.01\n",
    "    Stroop_model.scheduler.add_condition(color_hidden, When(converge, task, epsilon)))\n",
    "    Stroop_model.scheduler.add_condition(word_hidden, When(converge, task, epsilon)))\n",
    "\n",
    "PsyNeuLink provides a rich set of `pre-defined Conditions <Condition_Pre-Specified_List>` (such as ``When`` in the\n",
    "examples above), but Conditions can also be constructed using any Python function.  Together, these can be combined to\n",
    "construct virtually any schedule of execution that is logically possible.\n",
    "\n",
    ".. _BasicsAndPrimer_Control:\n",
    "\n",
    "Control\n",
    "~~~~~~~\n",
    "\n",
    "Another distinctive feature of PsyNeuLink is the ability to easily create models that include control;  that is,\n",
    "Mechanisms that can evaluate the output of other Mechanisms (or nested Compositions), and use this to regulate the\n",
    "processing of those Mechanisms.  For example, modifications of the ``Stroop_model`` shown below allow it to monitor\n",
    "conflict in the ``output`` Mechanism on each `trial <TimeScale.TRIAL>`, and use that to regulate the gain of the\n",
    "``task`` Mechanism::\n",
    "\n",
    "    # Construct control mechanism\n",
    "    control = ControlMechanism(name='CONTROL',\n",
    "                               objective_mechanism=ObjectiveMechanism(name='Conflict Monitor',\n",
    "                                                                      monitor=output,\n",
    "                                                                      function=Energy(input_shapes=2,\n",
    "                                                                                      matrix=[[0,-2.5],[-2.5,0]])),\n",
    "                               default_allocation=[0.5],\n",
    "                               control_signals=[(GAIN, task)])\n",
    "\n",
    "    # Construct the Composition using the control Mechanism as its controller:\n",
    "    Stroop_model = Composition(name='Stroop Model', controller=control)\n",
    "\n",
    "    # Print statement called by run method (below), that show state of Components after each trial\n",
    "    np.set_printoptions(precision=2)\n",
    "    global t\n",
    "    t = 0\n",
    "    def print_after():\n",
    "        global t\n",
    "        print(f'\\nEnd of trial {t}:')\n",
    "        print(f'\\t\\t\\t\\tcolor  word')\n",
    "        print(f'\\ttask:\\t\\t{task.value[0]}')\n",
    "        print(f'\\ttask gain:\\t   {task.parameter_ports[GAIN].value}')\n",
    "        print(f'\\t\\t\\t\\tred   green')\n",
    "        print(f'\\toutput:\\t\\t{output.value[0]}')\n",
    "        print(f'\\tdecision:\\t{decision.value[0]}{decision.value[1]}')\n",
    "        print(f'\\tconflict:\\t  {control.objective_mechanism.value[0]}')\n",
    "        t += 1\n",
    "\n",
    "    # Set up run and then execute it\n",
    "    task.initial_value = [0.5,0.5]         # Assign \"neutral\" starting point for task units on each trial\n",
    "    task.reset_stateful_function_when=AtTrialStart()  # Reset task units at beginning of each trial\n",
    "    num_trials = 4\n",
    "    stimuli = {color_input:[red]*num_trials,\n",
    "               word_input:[green]*num_trials,\n",
    "               task_input:[color]*num_trials}\n",
    "    Stroop_model.run(inputs=stimuli, call_after_trial=print_after)\n",
    "\n",
    "This example takes advantage of several additional features of PsyNeuLink, including its ability to automate certain\n",
    "forms of construction, and perform specified operations at various points during execution (e.g., reset variables\n",
    "and call user-defined functions).  For example, the constructor for the ControlMechanism can be used to specify how\n",
    "control should be configured, and automates the process of implementing it:  the **objective_mechanism** argument\n",
    "specifies the construction of an ObjectiveMechanism for the ControlMechanism that provides its input, and\n",
    "the **control_signals** argument specifies the parameters of the Mechanisms it should regulate and constructs the\n",
    "`ControlProjections <ControlProjection>` that implement this.  Furthermore, the constructor for the\n",
    "`ObjectiveMechanism` used in the **objective_mechanism** argument specifies that it should monitor the value of the\n",
    "``output`` Mechanism, and use the `Energy` Function to evaluate it.  PsyNeuLink automatically constructs the\n",
    "MappingProjections from ``output`` to the ObjectiveMechanism, and from the latter to the ControlMechanism.  The latter\n",
    "is then added to the ``Stroop_model`` as its `controller <Composition .controller>` in its constructor.\n",
    "The result is shown in the figure below, using the **show_controller** option of the Composition's `show_graph\n",
    "<ShowGraph.show_graph>` method:\n",
    "\n",
    ".. _BasicsAndPrimer_Stroop_Example_With_Control_Figure:\n",
    "\n",
    ".. figure:: _static/BasicsAndPrimer_Stroop_Model_Control.svg\n",
    "   :width: 50%\n",
    "\n",
    "   **Stroop Model with Controller.** Representation of the Composition with the ``control`` Mechanism added, generated\n",
    "   by a call to ``Stroop_model.show_graph(show_controller)``.\n",
    "\n",
    "The ``task`` Mechanism is configured to reset at the beginning of each `trial <TimeScale.TRIAL>`, and the\n",
    "**call_after_trial** argument of the Composition's `run <Composition.run>` method is used to print Mechanism values\n",
    "at the end of each `trial <TimeScale.TRIAL>` (see `below <Stroop_model_output>`).\n",
    "\n",
    "When the Composition executes, the Objective Mechanism receives the output of the ``output`` Mechanism, and uses the\n",
    "`Energy` function assigned to it to compute conflict in the ``output`` Mechanism (i.e., the degree of co-activity of\n",
    "the ``red`` and ``green`` values).  The result passed to the ``control`` Mechanism, which uses it to set the `gain\n",
    "<Logistic .gain>` of the ``task`` Mechanism's `Logistic` function.  The ``task`` Mechanism is configured to\n",
    "reset at the beginning of each `trial <TimeScale.TRIAL>`; and,since the ``control`` Mechanism was assigned as\n",
    "the Composition's `controller <Composition.controller>`, it executes at the end of each `trial <TimeScale.TRIAL>`\n",
    "after all of the other Mechanisms in the Composition have executed, which has its effects on the ``task`` Mechanism\n",
    "the next time it executes (i.e., on the next `trial <TimeScale.TRIAL>`;  a Composition's `controller\n",
    "<Composition.controller>` can also be configured to execute at the start of a `trial <TimeScale.TRIAL>`). Finally, the\n",
    "**call_after_trial** argument of the Composition's `run <Composition.run>` method is used to print Mechanism values\n",
    "at the end of each `trial <TimeScale.TRIAL>`.  The **animate** argument of the `run <Composition.run>` method can be\n",
    "used to generate an animation of the Composition's execution, as shown below:\n",
    "\n",
    ".. _BasicsAndPrimer_Stroop_Example_Animation_Figure:\n",
    "\n",
    ".. figure:: _static/BasicsAndPrimer_Stroop_Model_movie.gif\n",
    "   :width: 75%\n",
    "\n",
    "   **Animation of Stroop Model with Controller.** Generated by a call to ``Stroop_model.show_graph(show_controller)\n",
    "   with ``animate={\"show_controller\":True}`` in call to the `run <Composition.run>`.\n",
    "\n",
    "\n",
    "Running it for four `trials <TimeScale.TRIAL>` produces the following output::\n",
    "\n",
    "    .. _Stroop_model_output:\n",
    "\n",
    "    End of trial 0:\n",
    "                    color  word\n",
    "        task:\t\t[ 0.67  0.51]\n",
    "        task gain:\t   [ 0.5]\n",
    "                    red   green\n",
    "        output:\t\t[ 0.28  0.72]\n",
    "        decision:\t[-1.][ 2.36]\n",
    "        conflict:\t  [ 0.51]\n",
    "\n",
    "    End of trial 1:\n",
    "                    color  word\n",
    "        task:\t\t[ 0.81  0.4 ]\n",
    "        task gain:\t   [ 0.51]\n",
    "                    red   green\n",
    "        output:\t\t[ 0.38  0.62]\n",
    "        decision:\t[-1.][ 3.33]\n",
    "        conflict:\t  [ 0.59]\n",
    "\n",
    "    End of trial 2:\n",
    "                    color  word\n",
    "        task:\t\t[ 0.97  0.19]\n",
    "        task gain:\t   [ 0.59]\n",
    "                    red   green\n",
    "        output:\t\t[ 0.55  0.45]\n",
    "        decision:\t[ 1.][ 3.97]\n",
    "        conflict:\t  [ 0.62]\n",
    "\n",
    "    End of trial 3:\n",
    "                    color  word\n",
    "        task:\t\t[ 1.    0.04]\n",
    "        task gain:\t   [ 0.62]\n",
    "                    red   green\n",
    "        output:\t\t[ 0.65  0.35]\n",
    "        decision:\t[ 1.][ 2.95]\n",
    "        conflict:\t  [ 0.57]\n",
    "\n",
    "Notice that initially, because control starts out relatively low (``default_allocation=[0.5]``), the representation of\n",
    "the instruction in the ``task`` Mechanism (color = ``[1,0]``) is relatively weak (``[0.67, 0.51]``).  As a result,\n",
    "the model generates the incorrect response to the incongrent stimulus([-1] = green, rather than [1] = red), due to\n",
    "the stronger weights of the Projections in the ``word_pathway``.  However, beacuse this is associated with a moderate\n",
    "amount of conflict (``[0.51]``), control is increased on the next trial, which in turn increases the gain of the\n",
    "``task`` Mechanism, stengthening its representation of the instruction so that it eventually fully activates the\n",
    "color task and generates the correct response. A more elaborate example of this model can be found at\n",
    "`BotvinickConflictMonitoringModel`. More complicated forms of control are also possible, for example, ones that run\n",
    "internal simulations to optimize the amount of control to optimize some criterion (e.g,. maximize the\n",
    "`expected value of control <https://royalsocietypublishing.org/doi/full/10.1098/rstb.2013.0478>`_ (see XXX EVC\n",
    "script), or to implement `model-based learning <https://royalsocietypublishing.org/doi/full/10.1098/rstb.2013.0478>`_\n",
    "(see XXX LVOC script).\n",
    ".. FIX: ADD LINKS TO SCRIPTS ABOVE\n",
    "\n",
    ".. _BasicsAndPrimer_Parameters:\n",
    "\n",
    "Parameters\n",
    "~~~~~~~~~~\n",
    "Every Component has a set of parameters that determine how the Component operates, or that contain information about\n",
    "the state of its operation.  For example, every Component has a `value <Component_Value>` parameter that stores the\n",
    "result of the Component's `function <Component_Function>` after it has executed. (Note here the difference in the\n",
    "generic use of the term \"value\" to designate the quantity assigned to a parameter, and its use as the name of a\n",
    "*particular* parameter of a Component.)  Although parameters are attributes of a Component, and can be accessed like\n",
    "any other Python attribute (as described below), they are actually instances of a special `Parameters` class that\n",
    "supports a number of important features. These include the ability to simultaneously have different values in\n",
    "different contexts (often referred to as `\"statefulness\" <Parameter_Statefulness>`), the ability to keep a record of\n",
    "previous values, and the ability to be `modulated <ModulatorySignal_Modulation>` by other Components in PsyNeuLink.\n",
    "These features are supported by methods on the Parameter class, as described below.\n",
    "\n",
    ".. _BasicsAndPrimer_Accessing_Parameters:\n",
    "\n",
    "Accessing Parameter Values\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "The most recently assigned value of a parameter can be accessed like any other attrribute in Python,\n",
    "by using \"dot notation\" -- that is, ``<Component>.<parameter>``. For instance, the print statements in the\n",
    "``print_after`` function of the example above use ``output.value`` and ``decision.value`` to access the `value\n",
    "<Mechanism_Base.value>` parameter of the ``output`` and ``decision`` Mechanisms, respectively (more specifically, they\n",
    "access the first element of the output Mechanism's value, ``output.value[0]``, and the first and second elements of the\n",
    "decison Mechanism's `value <Mechanism_Base.value>`).  This returns their most recently assigned values. However, as an\n",
    "instance of the `Parameters` class, a parameter can be `stateful <Parameter.stateful>`, which means it can have more\n",
    "than one value associated with it. For example, PsyNeuLink has the capacity to execute the same Component in\n",
    "different `contexts <Parameter_Statefulness>`, either as part of different Compositions or, within the same\n",
    "Composition, as part of `model-based simulations <OptimizationControlMechanism_Model_Based>` executed by the\n",
    "Composition's `controller <Composition_Controller>`.  The value of a parameter in a particular context can be\n",
    "accessed by using the `get <Parameter.get>` method for the parameter and providing the context, for example::\n",
    "\n",
    "    >>> output.parameters.value.get('Stroop Model - Conflict Monitoring')[0]\n",
    "    [ 0.65  0.35]\n",
    "\n",
    "Notice that, here, the name of the Composition in which the Mechanism was executed is used as the context; see\n",
    "`Composition_Scope_of_Execution` for additional information about how to specify contexts.  If no context is specified,\n",
    "then the `get <Parameter.get>` method returns the most recently assigned value of the parameter, just like dot\n",
    "notation.\n",
    "\n",
    "If a parameter is `stateful <Parameter.stateful>`, then its previous value can also be accessed, using the\n",
    "`get_previous <Parameter.get_previous>` method; for example::\n",
    "\n",
    "    >>> output.parameters.value.get_previous('Stroop Model - Conflict Monitoring')[0]\n",
    "    [ 0.55  0.45]\n",
    "\n",
    "Notice that the value returned is the one from Trial 2 in the example above, immediately prior to the last one run\n",
    "(Trial 3).  By default, stateful parameters preserve only one previous value.  However, a parameter can be\n",
    "specified to have a longer `history <Parameter.history>`, in which case `get_previous <Parameter.get_previous>` can\n",
    "be used to access earlier values.  For example, the following sets output Mechanism's `value <Mechanism_Base.value>`\n",
    "parameter to store up to three previous values::\n",
    "\n",
    "    >>> output.parameters.value.history_max_length = 3\n",
    "\n",
    "If included in the script above, then the following would return the ``output`` Mechanism's `value\n",
    "<Mechanism_Base.value>` from two trials before the last one run::\n",
    "\n",
    "    >>> output.parameters.value.get_previous('Stroop Model - Conflict Monitoring', 2)[0]\n",
    "    [ 0.38 0.62]\n",
    "\n",
    "Notice that this is the value from Trial 1 in the example above.\n",
    "\n",
    ".. _BasicsAndPrimer_Setting_Parameters:\n",
    "\n",
    "Setting Parameter Values\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "There are two different ways to set parameter values in PsyNeuLink, that correspond to the two ways of accessing\n",
    "their values discussed in the preceding section:\n",
    "\n",
    "- \"dot notation\" -- that is, ``<Component>.<parameter> = some_new_value``\n",
    "- the `set <Parameter.set>` method of the Parameter class; that is\n",
    "  ``<Component>.parameters.<parameter>.set(<value>, <context>)``).\n",
    "\n",
    "In the case of `non-stateful parameters <Parameter_Statefulness>`, these two approaches behave equivalently, in which\n",
    "case it is easier and clearer to use dot notation.\n",
    "\n",
    ".. warning::\n",
    "    In most cases, non-stateful parameters are intended to be configured automatically for a component upon\n",
    "    instantiation by PsyNeuLink. Oftentimes, there are additional steps of validation and setup beyond simply\n",
    "    assigning a value to some attribute of the component instance. Be cautious if you decide to manually set a\n",
    "    non-stateful parameter.\n",
    "\n",
    "In the case of `stateful parameters <Parameter_Statefulness>` (the most common type), assigning a value using dot\n",
    "notation and the `set <Parameter.set>` method can behave differently under some circumstances.\n",
    "\n",
    "Dot notation is simpler, but assumes that the `context <Context>` for which the specified value should apply is the\n",
    "most recent context in which the Parameter's owner Component has executed.  If the value should apply to a different\n",
    "context, then the `set <Parameter.set>` method must be used, which allows explicit specification of the context.\n",
    "See below for examples of appropriate use cases for each, as well as explanations of contexts when executing\n",
    "`Mechanisms <Mechanism_Execution_Composition>` and `Compositions <Composition_Execution_Context>`, respectively.\n",
    "\n",
    ".. warning::\n",
    "   If a given component has not yet been executed, or has only been executed standalone without a user-provided context,\n",
    "   then dot notation will set the Parameter's value in the baseline context. Analogously, calling the `set\n",
    "   <Parameter.set>` method without providing a context will also set the Parameter's value in the baseline\n",
    "   context. In either of these cases, any new context in which the Component executes after that will use the\n",
    "   newly-provided value as the Parameter's baseline.\n",
    "\n",
    "Example use-cases for dot notation\n",
    "**********************************\n",
    "\n",
    "- Run a Composition, then change the value of a component's parameter, then run the Composition again with the new\n",
    "  parameter value:\n",
    "\n",
    "    >>> # instantiate the mechanism\n",
    "    >>> m = ProcessingMechanism(name='m')\n",
    "    >>> # create a Composition containing the Mechanism\n",
    "    >>> comp1 = Composition(name='comp1', nodes=[m])\n",
    "    >>> comp1.run(inputs={m:1})\n",
    "    >>> # returns: [array([1.])]\n",
    "    >>> # set slope of m1's function to 2 for the most recent context (which is now comp1)\n",
    "    >>> m.function.slope.base = 2\n",
    "    >>> comp1.run(inputs={m:1})\n",
    "    >>> # returns: [array([2.])]\n",
    "    >>> # note that changing the slope of m's function produced a different result\n",
    "    >>> comp2 = Composition(name='comp2', nodes=[m])\n",
    "    >>> comp2.run(inputs={m:1})\n",
    "    >>> # returns: [array([1.])]\n",
    "    >>> # because slope is a stateful parameter,\n",
    "    >>> # executing the Mechanism in a different context is unaffected by the change\n",
    "\n",
    "- Cautionary example: using dot notation before a component has been executed in a non-default context will change\n",
    "  its value in the default context, which will in turn propagate to new contexts in which it is executed:\n",
    "\n",
    "    >>> # instantiate the mechanism\n",
    "    >>> m = ProcessingMechanism(name='m')\n",
    "    >>> # create two Compositions, each containing m\n",
    "    >>> comp1 = Composition(name='comp1', nodes=[m])\n",
    "    >>> comp2 = Composition(name='comp2', nodes=[m])\n",
    "    >>> m.execute([1])\n",
    "    >>> # returns: [array([1.])]\n",
    "    >>> # executing m outside of a Composition uses its default context\n",
    "    >>> m.function.slope.base = 2\n",
    "    >>> m.execute([1])\n",
    "    >>> # returns: [array([2.])]\n",
    "    >>> comp1.run(inputs={m:1})\n",
    "    >>> # returns: [array([2.])]\n",
    "    >>> comp2.run(inputs={m:1})\n",
    "    >>> # returns: [array([2.])]\n",
    "    >>> # the change of the slope of m's function propagated to the new Contexts in which it executes\n",
    "\n",
    "Example use-cases for Parameter set method\n",
    "******************************************\n",
    "\n",
    "- Change the value of a parameter for a context that was not the last context in which the parameter's owner executed:\n",
    "\n",
    "    >>> # instantiate the mechanism\n",
    "    >>> m = ProcessingMechanism(name='m')\n",
    "    >>> # create two Compositions, each containing m\n",
    "    >>> comp1 = Composition(name='comp1', nodes=[m])\n",
    "    >>> comp2 = Composition(name='comp2', nodes=[m])\n",
    "    >>> comp1.run({m:[1]})\n",
    "    >>> # returns: [array([1.])]\n",
    "    >>> comp2.run({m:[1]})\n",
    "    >>> # returns: [array([1.])]\n",
    "    >>> # the last context in which m was executed was comp2,\n",
    "    >>> # so using dot notation to change a parameter of m would apply only for the comp2 context;\n",
    "    >>> # changing the parameter value for the comp1 context requires use of the set method\n",
    "    >>> m.function_parameters.slope.set(2, comp1)\n",
    "    >>> comp1.run({m:[1]})\n",
    "    >>> # returns: [array([2.])]\n",
    "    >>> comp2.run({m:[1]})\n",
    "    >>> # returns: [array([1.])]\n",
    "\n",
    "- Cautionary example: calling the set method of a parameter without a context specified changes its value in the\n",
    "  default context which, in turn, will propagate to new contexts in which it is executed:\n",
    "\n",
    "    >>> # instantiate the mechanism\n",
    "    >>> m = ProcessingMechanism(name='m')\n",
    "    >>> # create two Compositions, each containing m\n",
    "    >>> comp1 = Composition(name='comp1', nodes=[m])\n",
    "    >>> comp2 = Composition(name='comp2', nodes=[m])\n",
    "    >>> comp1.run({m:[1]})\n",
    "    >>> # returns: [array([1.])]\n",
    "    >>> comp2.run({m:[1]})\n",
    "    >>> # returns: [array([1.])]\n",
    "    >>> # calling the set method without specifying a context will change the default value for new contexts\n",
    "    >>> m.function_parameters.slope.set(2)\n",
    "    >>> comp1.run({m: [1]})\n",
    "    >>> # returns: [array([1.])]\n",
    "    >>> # no change because the context had already been set up before the call to set\n",
    "    >>> comp2.run({m:[1]})\n",
    "    >>> # returns: [array([1.])]\n",
    "    >>> # no change because the context had already been set up before the call to set\n",
    "    >>> # set up a new context\n",
    "    >>> comp3 = Composition(name='comp3', nodes=[m])\n",
    "    >>> comp3.run({m: [1]})\n",
    "    >>> # returns: [array([2.])]\n",
    "    >>> # 2 is now the default value for the slope of m's function, so it now produces a different result\n",
    "\n",
    "Function Parameters\n",
    "^^^^^^^^^^^^^^^^^^^\n",
    "The `parameters <Component_Parameters>` attribute of a Component contains a list of all of its parameters. It is\n",
    "important here to recognize the difference between the parameters of a Component and those of its `function\n",
    "<Component_Function>`.  In the examples above, `value <Component_Value>` is a parameter of the ``output`` and\n",
    "``decision`` Mechanisms themselves.  However, each of those Mechanisms also has a `function <Mechanism_Base\n",
    ".function>`; and, since those are PsyNeuLink `Functions <Function>` which are also Compoments, those too have\n",
    "parameters.  For example, the ``output`` Mechanism was assigned the `Logistic` `Function`, which has a `gain\n",
    "<Logistic.gain>` and a `bias <Logistic.bias>` parameter (as well as others).  The parameters of a Component's\n",
    "`function <Component_Function>` can also be accessed using dot notation, by referencing the function in the\n",
    "specification.  For example, the current value of the `gain <Logistic.gain>` parameter of the ``output``\\'s Logistic\n",
    "Function can be accessed in either of the following ways::\n",
    "\n",
    "    >>> output.function.gain.base\n",
    "    1.0\n",
    "    >>> output.function.parameters.gain.get()\n",
    "    1.0\n",
    "\n",
    "Modulable Parameters\n",
    "^^^^^^^^^^^^^^^^^^^^\n",
    "Some parameters of Components can be modulable, meaning they can be modified by another Component (specifically,\n",
    "a `ModulatorySignal <ModulatorySignal>` belonging to a `ModulatoryMechanism <ModulatoryMechanism>`).  If the parameter\n",
    "of a `Mechanism <Mechanism>` or a `Projection <Projection>` is modulable, it may be assigned a `ParameterPort` -- this is a\n",
    "Component that belongs to the Mechanism or Projection and can receive a Projection from a ModulatorySignal, allowing\n",
    "another component to modulate the value of the parameter. ParameterPorts are created for every modulable parameter of\n",
    "a Mechanism, its `function <Mechanism_Base.function>`, any of its\n",
    "secondary functions, and similarly for Projections.  These determine the value\n",
    "of the parameter that is actually used when the Component is executed, which may be different than the base value\n",
    "returned by accessing the parameter directly (as in the examples above); see `ModulatorySignal_Modulation` for a more\n",
    "complete description of modulation.  The current *modulated* value of a parameter can be accessed from the `value\n",
    "<ParameterPort.value>` of the corresponding ParameterPort.  For instance, the print statement in the example above\n",
    "used ``task.parameter_ports[GAIN].value`` to report the modulated value of the `gain <Logistic.gain>` parameter of\n",
    "the ``task`` Mechanism's `Logistic` function when the simulation was run.  For convenience, it is also possible to\n",
    "access the value of a modulable parameter via dot notation. Dot notation for modulable parameters is slightly different\n",
    "than for non-modulable parameters to provide easy access to both base and modulated values::\n",
    "\n",
    "    >>> task.function.gain\n",
    "    (Logistic Logistic Function-5):\n",
    "        gain.base: 1.0\n",
    "        gain.modulated: [0.55]\n",
    "\n",
    "Instead of just returning a value, the dot notation returns an object with `base` and `modulated` attributes.\n",
    "`modulated` refers to the `value <ParameterPort.value>` of the ParameterPort for the parameter::\n",
    "\n",
    "    >>> task.parameter_ports[GAIN].value\n",
    "    [0.62]\n",
    "    >>> task.gain.modulated\n",
    "    [0.62]\n",
    "\n",
    "This works for any modulable parameters of the Mechanism, its\n",
    "`function <Mechanism_Base.function>`, or secondary functions.  Note that,\n",
    "here, neither the ``parameters`` nor the ``function`` attributes of the Mechanism need to be included in the reference.\n",
    "Note also that, as explained above, the value returned is different from the base value of the function's gain\n",
    "parameter::\n",
    "\n",
    "    >>> task.function.gain.base\n",
    "    1.0\n",
    "\n",
    "This is because when the Compoistion was run, the ``control`` Mechanism modulated the value of the gain parameter.\n",
    "\n",
    "Some Parameters may be modulable, but not *modulated* by a\n",
    "ParameterPort, because ParameterPorts are only created for Parameters\n",
    "whose values are numeric at the time of Component construction. For\n",
    "example, `TransferMechanism.termination_threshold` has a default value\n",
    "of None and will not have a ParameterPort by default. Similarly, when\n",
    "`noise <TransferMechanism.noise>` is set to a function or value\n",
    "containing a function, it will not have an associated ParameterPort.\n",
    "\n",
    "In this case, dot notation for these modulable Parameters will behave\n",
    "the same as for non-modulable Parameters.\n",
    "\n",
    ".. *Initialization* ???XXX\n",
    "\n",
    ".. _BasicsAndPrimer_Monitoring_Values:\n",
    "\n",
    "Displaying and Logging Values\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "*Displaying values* The console output in the example above was generated using the **call_after_trial** argument in\n",
    "the Composition's `run <Composition.run>` method, that calls the ``print_after`` function defined in Python.  There\n",
    "are other similar \"hooks\" in the `run <Composition.run>` method that can be used not only to monitor values, but also\n",
    "to carry out custom operations at various points during execution (before and/or after each `run <TimeScale.RUN>`,\n",
    "`run <TimeScale.TRIAL>` or execution of the Components in a trial).\n",
    "\n",
    "*Logging values*. PsyNeuLink also has powerful logging capabilities that can be used to track and report any\n",
    "parameter of a model.  For example, including the following lines in the script for ``Stroop_model``,  after the\n",
    "``task`` and ``control`` Mechanisms are constructed::\n",
    "\n",
    "    task.log.set_log_conditions(VALUE)\n",
    "    control.log.set_log_conditions(VARIABLE)\n",
    "    control.log.set_log_conditions(VALUE)\n",
    "\n",
    "logs the value of the ``control`` and ``task`` Mechanisms each time they are executed.  Information in the log can be\n",
    "printed to the console using its `print_entries <Log.print_entries>` method, and specifying the desired information\n",
    "in its **display** argument.  For example, calling the following after ``Stroop_model.run`` has been called::\n",
    "\n",
    "    Stroop_model.log.print_entries(display=[TIME, VALUE])\n",
    "\n",
    "generates the following report of the time at which the ``control`` and ``task`` Mechanisms were executed and their\n",
    "value for each execution (only the first two trials worth of the output are reproduced here)::\n",
    "\n",
    "    Log for Stroop Model:\n",
    "\n",
    "    Logged Item:   Time          Value\n",
    "\n",
    "    'CONTROL'      0:0:10:0     [[0.51]]\n",
    "    'CONTROL'      0:1:10:0     [[0.59]]\n",
    "    ...\n",
    "\n",
    "    'TASK'         0:0:0:1      [[0.57 0.56]]\n",
    "    'TASK'         0:0:1:1      [[0.58 0.55]]\n",
    "    'TASK'         0:0:2:1      [[0.59 0.55]]\n",
    "    'TASK'         0:0:3:1      [[0.6  0.54]]\n",
    "    'TASK'         0:0:4:1      [[0.61 0.54]]\n",
    "    'TASK'         0:0:5:1      [[0.62 0.53]]\n",
    "    'TASK'         0:0:6:1      [[0.63 0.53]]\n",
    "    'TASK'         0:0:7:1      [[0.64 0.52]]\n",
    "    'TASK'         0:0:8:1      [[0.65 0.51]]\n",
    "    'TASK'         0:0:9:1      [[0.67 0.51]]\n",
    "    'TASK'         0:1:0:1      [[0.68 0.5 ]]\n",
    "    'TASK'         0:1:1:1      [[0.69 0.49]]\n",
    "    'TASK'         0:1:2:1      [[0.71 0.48]]\n",
    "    'TASK'         0:1:3:1      [[0.72 0.47]]\n",
    "    'TASK'         0:1:4:1      [[0.74 0.46]]\n",
    "    'TASK'         0:1:5:1      [[0.75 0.45]]\n",
    "    'TASK'         0:1:6:1      [[0.77 0.44]]\n",
    "    'TASK'         0:1:7:1      [[0.78 0.42]]\n",
    "    'TASK'         0:1:8:1      [[0.8  0.41]]\n",
    "    'TASK'         0:1:9:1      [[0.81 0.4 ]]\n",
    "    ...\n",
    "\n",
    "The time is reported as run:trial:pass:time_step.  Note that there is only one entry for the ``control`` Mechanism\n",
    "per trial, since it is executed only once per trial; but there are ten entries for the ``task`` Mechanism for each\n",
    "trial since it executed ten times, as specified in the Conditions described above.\n",
    "\n",
    "The output of a `Log` can also be reported in various other formats, including as a `numpy <https://docs.scipy\n",
    ".org/doc/numpy/reference/generated/numpy.array.html>`_ array (using its `nparray <Log.nparray>` method, as a\n",
    "dictionary of values for each entry (using its `nparray_dictionary <Log.nparray_dictionary>` method), and in `CSV\n",
    "<https://en.wikipedia.org/wiki/Comma-separated_values>`_ format (using its `csv <Log.csv>` method.\n",
    "\n",
    ".. _BasicsAndPrimer_Learning:\n",
    "\n",
    "Learning\n",
    "~~~~~~~~\n",
    "\n",
    "Needless to say, no framework for modeling brain and/or cognitive function is complete without implementing learning\n",
    "mechanisms.  PsyNeuLink does so in two ways: in a native form, and by integrating tools available from other\n",
    "Python-based environments.  Currently, PsyNeuLink has builtin intregration with `PyTorch <https://pytorch.org>`_,\n",
    "however other envirnoments can be accessed using `UserDefinedFunctions <UserDefinedFunction>`.  Since such\n",
    "environments are becoming increasingly accessible and powerful, the native implementation of learning in PsyNeuLink\n",
    "is designed with a complementary set of goals: modularity and exposition, rather than efficiency of computation.\n",
    "That is, it is better suited for \"story-boarding\" a model that includes learning components, and for illustrating\n",
    "process flow during learning, than it is for large scale simulations involving learning.  However, the specification\n",
    "of the learning components of a model in PsyNeuLink can easily be translated into a Pytorch description, which can\n",
    "then be integrated into the PsyNeuLink model with all the benefits of Pytorch execution.  Each of the two ways of\n",
    "specifying learning components is described below.\n",
    "\n",
    "LearningMechanisms\n",
    "^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "PsyNeuLink has a native class -- `LearningMechanism` -- that can be used to implement various forms of learning,\n",
    "including unsupervised forms (such as `Hebbian`) and supervised forms (such as reinforcement learning and\n",
    "backpropagation). LearningMechanisms take as their input a target and/or an error signal, provided by a\n",
    "`MappingProjection` from the source of the error signal (either a ComparatorMechanism or another LearningMechanism).\n",
    "LearningMechanisms use `LearningSignals` (a type of `OutputPort`) to send a `LearningProjection` to the\n",
    "`MappingProjection` that is being learned.  The type of learning implemented by a LearningMechanism is determined by\n",
    "the class of `LearningFunction <LearningFunctions>` assigned as its `function <LearningMechanism.function>`.  In some\n",
    "cases (such as multilayered backpropagation networks), configuration of the LearningMechanisms and corresponding\n",
    "Projections can become complex; PsyNeuLink provides methods for implementing these automatically, which also serves\n",
    "to illustrate the flow of signals and errors implemented by the algorithm.  The example below implements learning in\n",
    "a simple three-layered neural network that learns to compute the X-OR operation::\n",
    "\n",
    "    # Construct Processing Mechanisms and Projections:\n",
    "    input = ProcessingMechanism(name='Input', default_variable=np.zeros(2))\n",
    "    hidden = ProcessingMechanism(name='Hidden', default_variable=np.zeros(10), function=Logistic())\n",
    "    output = ProcessingMechanism(name='Output', default_variable=np.zeros(1), function=Logistic())\n",
    "    input_weights = MappingProjection(name='Input Weights', matrix=np.random.rand(2,10))\n",
    "    output_weights = MappingProjection(name='Output Weights', matrix=np.random.rand(10,1))\n",
    "    xor_comp = Composition('XOR Composition')\n",
    "    learning_components = xor_comp.add_backpropagation_learning_pathway(\n",
    "                                                    pathway=[input, input_weights, hidden, output_weights, output])\n",
    "    target = learning_components[TARGET_MECHANISM]\n",
    "\n",
    "    # Create inputs:            Trial 1  Trial 2  Trial 3  Trial 4\n",
    "    xor_inputs = {'stimuli':[[0, 0],  [0, 1],  [1, 0],  [1, 1]],\n",
    "                  'targets':[  [0],     [1],     [1],     [0] ]}\n",
    "    xor_comp.learn(inputs={input:xor_inputs['stimuli'],\n",
    "                         target:xor_inputs['targets']})\n",
    "\n",
    "Calling the Composition's `show_graph <ShowGraph.show_graph>` with ``show_learning=True`` shows the network along with\n",
    "all of the learning components created by the call to ``add_backpropagation_pathway``:\n",
    "\n",
    ".. _BasicsAndPrimer_XOR_MODEL_Figure:\n",
    "\n",
    ".. figure:: _static/BasicsAndPrimer_XOR_Model_fig.svg\n",
    "   :width: 100%\n",
    "\n",
    "   **XOR Model.**  Items in orange are learning components implemented by the call to ``add_backpropagation_pathway``;\n",
    "   diamonds represent MappingProjections, shown as nodes so that the `LearningProjections` to them can be shown.\n",
    "\n",
    "\n",
    "Training the model requires specifying a set of inputs and targets to use as training stimuli, and identifying the\n",
    "target Mechanism (that receives the input specifying the target responses)::\n",
    "\n",
    "    # Construct 4 trials worth of stimuli and responses (for the four conditions of the XOR operation):\n",
    "    xor_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    xor_targets = np.array([ [0],   [1],     [1],    [0]])\n",
    "\n",
    "    # Identify target Mechanism returned by add_backpropation_pathway called above\n",
    "    target_mech = learning_components[TARGET_MECHANISM]\n",
    "\n",
    "    # Run the model:\n",
    "    result = xor_model.learn(inputs={input_mech:xor_inputs,\n",
    "                                   target_mech:xor_targets},\n",
    "                           num_trials=2)\n",
    "\n",
    "It can also be run without learning by calling the run method with ``enable_learning=False``.\n",
    "\n",
    ".. _BasicsAndPrimer_Rumelhart_Model:\n",
    "\n",
    "The model shown above implements learning for a simple linear path.  However, virtually any model can be created\n",
    "using calls to a Composition's `learning methods <Composition_Learning_Methods>` to build up more complex pathways.\n",
    "For example, the following implements a network for learning semantic representations described in\n",
    "`Rumelhart & Todd, 1993 <https://psycnet.apa.org/record/1993-97600-001>`_ (`pdf <https://web.stanford\n",
    ".edu/class/psych209a/ReadingsByDate/02_08/RumelhartTodd93.pdf>`_)::\n",
    "\n",
    "\n",
    "    #  Represention  Property  Quality  Action\n",
    "    #           \\________\\_______/_______/\n",
    "    #                        |\n",
    "    #                 Relations_Hidden\n",
    "    #                   _____|_____\n",
    "    #                  /           \\\n",
    "    #   Representation_Hidden  Relations_Input\n",
    "    #               /\n",
    "    #   Representation_Input\n",
    "\n",
    "    # Construct Mechanisms\n",
    "    rep_in = pnl.ProcessingMechanism(input_shapes=10, name='REP_IN')\n",
    "    rel_in = pnl.ProcessingMechanism(input_shapes=11, name='REL_IN')\n",
    "    rep_hidden = pnl.ProcessingMechanism(input_shapes=4, function=Logistic, name='REP_HIDDEN')\n",
    "    rel_hidden = pnl.ProcessingMechanism(input_shapes=5, function=Logistic, name='REL_HIDDEN')\n",
    "    rep_out = pnl.ProcessingMechanism(input_shapes=10, function=Logistic, name='REP_OUT')\n",
    "    prop_out = pnl.ProcessingMechanism(input_shapes=12, function=Logistic, name='PROP_OUT')\n",
    "    qual_out = pnl.ProcessingMechanism(input_shapes=13, function=Logistic, name='QUAL_OUT')\n",
    "    act_out = pnl.ProcessingMechanism(input_shapes=14, function=Logistic, name='ACT_OUT')\n",
    "\n",
    "    # Construct Composition\n",
    "    comp = Composition(name='Rumelhart Semantic Network')\n",
    "    comp.add_backpropagation_learning_pathway(pathway=[rel_in, rel_hidden])\n",
    "    comp.add_backpropagation_learning_pathway(pathway=[rel_hidden, rep_out])\n",
    "    comp.add_backpropagation_learning_pathway(pathway=[rel_hidden, prop_out])\n",
    "    comp.add_backpropagation_learning_pathway(pathway=[rel_hidden, qual_out])\n",
    "    comp.add_backpropagation_learning_pathway(pathway=[rel_hidden, act_out])\n",
    "    comp.add_backpropagation_learning_pathway(pathway=[rep_in, rep_hidden, rel_hidden])\n",
    "    comp.show_graph(show_learning=True)\n",
    "\n",
    "The figure below shows this network with all of its `learning components <Composition_Learning_Components>`:\n",
    "\n",
    ".. _BasicsAndPrimer_Rumelhart_Network_Figure:\n",
    "\n",
    ".. figure:: _static/BasicsAndPrimer_Rumelhart_Network.svg\n",
    "   :width: 75%\n",
    "\n",
    "   **Rumelhart Semantic Network.**  Items in orange are learning components implemented by the calls to\n",
    "   ``add_backpropagation_pathway``; diamonds represent MappingProjections, shown as nodes so that the\n",
    "   `LearningProjections` to them can be shown.\n",
    "\n",
    ".. XXX ADD REFERENCE TO Rumelhart Semantic Network Model once implemented\n",
    "\n",
    "Given the number of learning components, training the model above using standard PsyNeuLink components can take a\n",
    "considerable amount of time.  However, the same Composition can be implemented using the `AutodiffComposition`, by\n",
    "replacing the relevant line in the example above with ``comp = AutoComposition(name='Rumelhart Semantic Network')``).\n",
    "The AutodiffComposition uses `PyTorch <https://pytorch.org>`_ to execute learning, which runs considerably (as much as\n",
    "three orders of magnitude) faster (see `Composition_Learning`, as well as `Composition_Learning_AutodiffComposition`\n",
    "for comparisons of the advantages and disadvantages of using a standard `Composition` vs. `AutodiffComposition` for\n",
    "learning).\n",
    "\n",
    ".. _BasicsAndPrimer_Customization:\n",
    "\n",
    "Customization\n",
    "~~~~~~~~~~~~~\n",
    "\n",
    "The Mechanisms in the examples above all use PsyNeuLink `Functions`.  However, as noted earlier, a Mechanism can be\n",
    "assigned any Python function, so long as it is compatible with the Mechanism's type.  More specifically, its\n",
    "first argument must accept a variable that has the same shape as the Mechanism's variable.  For most Mechanism types\n",
    "this can be specified in the **default_variable** argument of their constructors, so in practice this places little\n",
    "constraint on the type of functions that can be assigned.  For example, the script below defines a function that\n",
    "returns the amplitude of a sinusoid with a specified frequency at a specified time, and then assigns this to a\n",
    "`ProcessingMechanism`::\n",
    "\n",
    "        >>> def my_sinusoidal_fct(input=[[0],[0]],\n",
    "        ...                       phase=0,\n",
    "        ...                       amplitude=1):\n",
    "        ...    frequency = input[0]\n",
    "        ...    time = input[1]\n",
    "        ...    return amplitude * np.sin(2 * np.pi * frequency * time + phase)\n",
    "        >>> my_wave_mech = pnl.ProcessingMechanism(default_variable=[[0],[0]],\n",
    "        ...                                        function=my_sinusoidal_fct)\n",
    "\n",
    "Note that the first argument is specified as a 2d variable that contains the frequency and time values as its elements\n",
    "-- this matches the definition of the ProcessingMechanism's **default_variable**, which the Mechanism will expect to\n",
    "receive as input from any other Mechanisms that project to it.  When a Python function is specified as the\n",
    "`function <Mechanism_Base.function>` of Mechanism (or any other Component in PsyNeuLink), it is automatically\n",
    "\"wrapped\" as a `UserDefinedFunction`, a special class of PsyNeuLink `Function <Functions>` that integrates it with\n",
    "PsyNeuLink:  in addition to making its first argument available as the input to the Component to which it is assigned,\n",
    "it also makes its parameters available for modulation by `ControlMechanisms <ControlMechanism>`.  For example,\n",
    "notice that ``my_sinusoidal_fct`` has two other arguments, in addition to its ``input``: ``phase`` and ``amplitude``.\n",
    "As a result, the phase and amplitude of ``my_wave_mech`` can be modulated in by referencing them in the constructor of a\n",
    "`ControlMechanism`::\n",
    "\n",
    "    >>> control = ControlMechanism(control_signals=[('phase', my_wave_mech),\n",
    "                                                     'amplitude', my_wave_mech])\n",
    "\n",
    "This facility not only makes PsyNeuLink flexible, but can be used to extend it in powerful ways.  For example, as\n",
    "mentioned under `BasicsAndPrimer_Learning`, functions from other environments that implement complex learning models\n",
    "can be assigned as the `function <Mechanism_Base.function>` of a Mechanism, and in that way integrated into a\n",
    "PsyNeuLink model.\n",
    "\n",
    "Conclusion\n",
    "~~~~~~~~~~\n",
    "\n",
    "The examples above are intended to provide a sample of PsyNeuLink's capabilities, and how they can be used.  The\n",
    "`Tutorial` provides a more thorough, interactive introduction to its use, and the `User's Guide <UserGuide>` provides\n",
    "a more detailed description of PsyNeuLink's organization and capabilities."
   ],
   "id": "488c33cedac40ccd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
